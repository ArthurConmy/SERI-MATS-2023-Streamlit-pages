{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Prompts\n",
    "\n",
    "This is the notebook I use to test out the functions in this directory, and generate the plots in the Streamlit page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as t\n",
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"gpt2-small\",\n",
    "#     center_unembed=True,\n",
    "#     center_writing_weights=True,\n",
    "#     fold_ln=True,\n",
    "#     device=\"cpu\"\n",
    "#     # refactor_factored_attn_matrices=True,\n",
    "# )\n",
    "# model.set_use_split_qkv_input(False)\n",
    "# model.set_use_attn_result(True)\n",
    "\n",
    "# t.save(model.half(), \"gpt2-small.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "import gzip\n",
    "\n",
    "from generate_html import (\n",
    "    CSS,\n",
    "    generate_4_html_plots,\n",
    "    generate_html_for_DLA_plot,\n",
    "    generate_html_for_logit_plot,\n",
    "    generate_html_for_loss_plot,\n",
    "    generate_html_for_unembedding_components_plot,\n",
    "    attn_filter,\n",
    "    _get_color,\n",
    ")\n",
    "from model_results import (\n",
    "    get_model_results,\n",
    "    ModelResults\n",
    ")\n",
    "from explore_prompts_utils import (\n",
    "    parse_str,\n",
    "    parse_str_tok_for_printing,\n",
    "    ST_HTML_PATH\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cpu\"\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openwebtext-10k (/home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8515ba128eb44c4ea924e5811e24db8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 60]) \n",
      "\n",
      "['<|endoftext|>', 'Oh', ' boy', ' was', ' this', ' damn', ' hard', ' to', ' crack', '.', '\\n', '\\n', 'Ok', ',', ' I', ' believe', ' before', ' it', ' was', ' established', ' before', ' that', ' A', 'perture', ' Science', ' headquarters', ' are', ' in', ' Cleveland', ',', ' OH', '.', '\\n', '\\n', 'Source', ':', ' HL', '2', 'EP', '2', '\\n', '\\n', 'Though', ',', ' this', ' has', ' been', ' found', '.', '\\n', '\\n', 'Source', ':', ' Portal', ' 2', '\\n', '\\n', 'It', ' can', ' be']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "SEQ_LEN = 60 # 1024\n",
    "\n",
    "DATA_STR = get_webtext(seed=6)[:BATCH_SIZE]\n",
    "DATA_STR = [parse_str(s) for s in DATA_STR]\n",
    "\n",
    "DATA_TOKS = model.to_tokens(DATA_STR)\n",
    "DATA_STR_TOKS = model.to_str_tokens(DATA_STR)\n",
    "\n",
    "if SEQ_LEN < 1024:\n",
    "    DATA_TOKS = DATA_TOKS[:, :SEQ_LEN]\n",
    "    DATA_STR_TOKS = [str_toks[:SEQ_LEN] for str_toks in DATA_STR_TOKS]\n",
    "\n",
    "DATA_STR_TOKS_PARSED = [[parse_str_tok_for_printing(str_tok) for str_tok in str_toks] for str_toks in DATA_STR_TOKS]\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "\n",
    "print(DATA_TOKS.shape, \"\\n\")\n",
    "\n",
    "print(DATA_STR_TOKS[0])\n",
    "\n",
    "batch_idx = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 82.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS\n",
      "-> (0, '10.7', 'mean, direct', True)\n",
      "-> (0, '10.7', 'mean, direct', False)\n",
      "-> (0, '10.7', 'zero, direct', True)\n",
      "-> (0, '10.7', 'zero, direct', False)\n",
      "-> (0, '10.7', 'mean, patched', True)\n",
      "-> (0, '10.7', 'mean, patched', False)\n",
      "-> (0, '10.7', 'zero, patched', True)\n",
      "-> (0, '10.7', 'zero, patched', False)\n",
      "-> (0, '11.10', 'mean, direct', True)\n",
      "-> (0, '11.10', 'mean, direct', False)\n",
      "-> (0, '11.10', 'zero, direct', True)\n",
      "-> (0, '11.10', 'zero, direct', False)\n",
      "-> (0, '11.10', 'mean, patched', True)\n",
      "-> (0, '11.10', 'mean, patched', False)\n",
      "-> (0, '11.10', 'zero, patched', True)\n",
      "-> (0, '11.10', 'zero, patched', False)\n",
      "LOGITS_ORIG\n",
      "-> (0,)\n",
      "LOGITS_ABLATED\n",
      "-> (0, '10.7', 'mean, direct')\n",
      "-> (0, '10.7', 'zero, direct')\n",
      "-> (0, '10.7', 'mean, patched')\n",
      "-> (0, '10.7', 'zero, patched')\n",
      "-> (0, '11.10', 'mean, direct')\n",
      "-> (0, '11.10', 'zero, direct')\n",
      "-> (0, '11.10', 'mean, patched')\n",
      "-> (0, '11.10', 'zero, patched')\n",
      "DLA\n",
      "-> (0, '10.7', 'neg')\n",
      "-> (0, '10.7', 'pos')\n",
      "-> (0, '11.10', 'neg')\n",
      "-> (0, '11.10', 'pos')\n",
      "ATTN\n",
      "-> (0, '10.7', 'Large', 'standard')\n",
      "-> (0, '10.7', 'Large', 'info-weighted')\n",
      "-> (0, '10.7', 'Small', 'standard')\n",
      "-> (0, '10.7', 'Small', 'info-weighted')\n",
      "-> (0, '11.10', 'Large', 'standard')\n",
      "-> (0, '11.10', 'Large', 'info-weighted')\n",
      "-> (0, '11.10', 'Small', 'standard')\n",
      "-> (0, '11.10', 'Small', 'info-weighted')\n",
      "UNEMBEDDINGS\n",
      "-> (0, '10.7')\n",
      "-> (0, '11.10')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
       "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@350&display=swap\" rel=\"stylesheet\">\n",
       "\n",
       "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
       "\n",
       "<style>\n",
       "body {\n",
       "    font-family: 'Source Sans 3', sans-serif;\n",
       "}\n",
       "\n",
       "table {\n",
       "    border-collapse: collapse;\n",
       "    width: 100%;\n",
       "}\n",
       "th, td {\n",
       "    border: 1px solid black;\n",
       "    padding: 6px;\n",
       "    text-align: left;\n",
       "    line-height: 0.8em;\n",
       "}\n",
       ".empty-row td {\n",
       "    border: none;\n",
       "}\n",
       "mark {\n",
       "    font-size: 1rem;\n",
       "    line-height: 1.48em;\n",
       "}\n",
       "\n",
       ".tooltip {\n",
       "    position: relative;\n",
       "    display: inline-block;\n",
       "    cursor: pointer;\n",
       "}\n",
       ".tooltip .tooltiptext {\n",
       "    font-size: 0.9rem;\n",
       "    min-width: 275px;\n",
       "    visibility: hidden;\n",
       "    background-color: #eee;\n",
       "    color: #000;\n",
       "    text-align: center;\n",
       "    padding: 5px;\n",
       "    position: absolute;\n",
       "    z-index: 1;\n",
       "    top: 125%;\n",
       "    left: 50%;\n",
       "    margin-left: -10px;\n",
       "    opacity: 0;\n",
       "    transition: opacity 0.0s;\n",
       "}\n",
       "\n",
       ".tooltip:hover .tooltiptext {\n",
       "    visibility: visible;\n",
       "    opacity: 1;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "$(document).ready(function(){\n",
       "  $('.tooltip').hover(function(){\n",
       "    var tooltipWidth = $(this).children('.tooltiptext').outerWidth();\n",
       "    var viewportWidth = $(window).width();\n",
       "    var tooltipRight = $(this).offset().left + tooltipWidth;\n",
       "    if (tooltipRight > viewportWidth) {\n",
       "      $(this).children('.tooltiptext').css('left', 'auto').css('right', '0');\n",
       "    }\n",
       "  }, function() {\n",
       "    $(this).children('.tooltiptext').css('left', '50%').css('right', 'auto');\n",
       "  });\n",
       "});\n",
       "</script>\n",
       "<td>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(243, 245, 246);opacity:1.0;line-height:1.75em\"><font color=\"black\"><|endoftext|></font></mark><span class=\"tooltiptext\"><b>'<|endoftext|>'</b><br>0.0063</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(248, 243, 239);opacity:1.0;line-height:1.75em\"><font color=\"black\">All</font></mark><span class=\"tooltiptext\"><b>'All'</b><br>-0.0087</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(222, 235, 242);opacity:1.0;line-height:1.75em\"><font color=\"black\">'s</font></mark><span class=\"tooltiptext\"><b>''s'</b><br>0.0364</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(244, 246, 246);opacity:1.0;line-height:1.75em\"><font color=\"black\">&nbsp;fair</font></mark><span class=\"tooltiptext\"><b>' fair'</b><br>0.0039</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(5, 48, 97);opacity:1.0;line-height:1.75em\"><font color=\"white\">&nbsp;in</font></mark><span class=\"tooltiptext\"><b>' in'</b><br>0.2709</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 246, 245);opacity:1.0;line-height:1.75em\"><font color=\"black\">&nbsp;love</font></mark><span class=\"tooltiptext\"><b>' love'</b><br>-0.0024</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(118, 180, 213);opacity:1.0;line-height:1.75em\"><font color=\"black\">&nbsp;and</font></mark><span class=\"tooltiptext\"><b>' and'</b><br>0.1273</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247);opacity:1.0;line-height:1.75em\"><font color=\"black\">&nbsp;war</font></mark><span class=\"tooltiptext\"></span></span>&nbsp;</td><br><br><br><br><br><br><br><br><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"All's fair in love and war\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "MODEL_RESULTS = get_model_results(model, toks, NEGATIVE_HEADS)\n",
    "\n",
    "HTML_PLOTS = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = toks,\n",
    "    data_str_toks_parsed = str_toks_parsed,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = False,\n",
    ")\n",
    "\n",
    "for k, v in HTML_PLOTS.items():\n",
    "    print(k)\n",
    "    for k2 in v.keys(): print(f\"-> {k2}\")\n",
    "\n",
    "display(HTML(CSS + HTML_PLOTS[\"LOSS\"][(0, \"10.7\", \"mean, direct\", True)] + \"<br>\" * 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits ablated and attn are by far the worst offenders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS = get_model_results(model, DATA_TOKS, negative_heads = NEGATIVE_HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_4_html_plots(\n",
    "    model: HookedTransformer,\n",
    "    data_toks: Float[Int, \"batch seq_len\"],\n",
    "    data_str_toks_parsed: List[List[str]],\n",
    "    negative_heads: List[Tuple[int, int]] = NEGATIVE_HEADS,\n",
    "    save_files: bool = False,\n",
    "    model_results: Optional[ModelResults] = None,\n",
    ") -> Dict[str, Dict[Tuple, str]]:\n",
    "    '''\n",
    "    Generates all the HTML plots for the Streamlit page. \n",
    "\n",
    "    This is called by me in `explore_prompts.ipynb`, to get data for open webtext.\n",
    "\n",
    "    It's also called in the Streamlit page, to get data for the user's input.\n",
    "\n",
    "    The output is in the form of nested dicts. Each key is a type of plot (e.g. the loss plot, or the logit attribution\n",
    "    plot). Each value is itself a dict, containing all of these plots for different batch indices / types of ablation etc.\n",
    "    '''\n",
    "    HTML_PLOTS = {\n",
    "        \"LOSS\": {},\n",
    "        \"LOGITS_ORIG\": {},\n",
    "        \"LOGITS_ABLATED\": {},\n",
    "        \"DLA\": {},\n",
    "        \"ATTN\": {},\n",
    "        \"UNEMBEDDINGS\": {}\n",
    "    }\n",
    "\n",
    "    BATCH_SIZE = data_toks.shape[0]\n",
    "\n",
    "    if model_results is None:\n",
    "        MODEL_RESULTS = get_model_results(model, data_toks, negative_heads = negative_heads)\n",
    "    else:\n",
    "        MODEL_RESULTS = model_results\n",
    "\n",
    "    # ! (1) Calculate the loss diffs from ablating\n",
    "\n",
    "    loss_diffs = t.stack([\n",
    "        t.stack(list(MODEL_RESULTS.loss.mean_direct.data.values())),\n",
    "        t.stack(list(MODEL_RESULTS.loss.zero_direct.data.values())),\n",
    "        t.stack(list(MODEL_RESULTS.loss.mean_patched.data.values())),\n",
    "        t.stack(list(MODEL_RESULTS.loss.zero_patched.data.values())),\n",
    "    ]) - MODEL_RESULTS.loss_orig\n",
    "\n",
    "    for batch_idx in tqdm(range(BATCH_SIZE)):\n",
    "        for head_idx, (layer, head) in enumerate(negative_heads):\n",
    "            head_name = f\"{layer}.{head}\"\n",
    "\n",
    "            # Calculate the loss diffs (and pad them with zero at the end, cause we don't know!)\n",
    "            # shape is (ablation_type=4, batch, seq)\n",
    "            loss_diffs_padded = t.concat([loss_diffs[:, head_idx], t.zeros((4, BATCH_SIZE, 1))], dim=-1)\n",
    "\n",
    "            # For each different type of ablation, get the loss diffs\n",
    "            for loss_diff, ablation_type in zip(loss_diffs_padded, [\"mean, direct\", \"zero, direct\", \"mean, patched\", \"zero, patched\"]):\n",
    "                html_25, html_max = generate_html_for_loss_plot(\n",
    "                    data_str_toks_parsed[batch_idx],\n",
    "                    loss_diff = loss_diff[batch_idx],\n",
    "                )\n",
    "                HTML_PLOTS[\"LOSS\"][(batch_idx, head_name, ablation_type, True)] = str(html_max)\n",
    "                HTML_PLOTS[\"LOSS\"][(batch_idx, head_name, ablation_type, False)] = str(html_25)\n",
    "\n",
    "    # ! (2, 3, 4) Calculate the logits & direct logit attributions\n",
    "\n",
    "    token_log_probs_dict = {\n",
    "        \"orig\": MODEL_RESULTS.logits_orig.log_softmax(-1),\n",
    "        **{\n",
    "            f\"{ablation_type}, {layer}.{head}\": getattr(MODEL_RESULTS.logits, ablation_type.replace(\", \", \"_\"))[layer, head].log_softmax(-1)\n",
    "            for layer, head in negative_heads\n",
    "            for ablation_type in [\"mean, direct\", \"zero, direct\", \"mean, patched\", \"zero, patched\"]\n",
    "        }\n",
    "    }\n",
    "    token_log_probs_top10_dict = {\n",
    "        k: v.topk(10, dim=-1)\n",
    "        for (k, v) in token_log_probs_dict.items()\n",
    "    }\n",
    "    direct_effect_log_probs_dict = {\n",
    "        (layer, head): MODEL_RESULTS.direct_effect[layer, head].log_softmax(-1)\n",
    "        for layer, head in negative_heads\n",
    "    }\n",
    "\n",
    "    for batch_idx in tqdm(range(BATCH_SIZE)):\n",
    "\n",
    "        html_orig = generate_html_for_logit_plot(\n",
    "            data_toks,\n",
    "            token_log_probs_dict[\"orig\"],\n",
    "            token_log_probs_top10_dict[\"orig\"],\n",
    "            token_log_probs_dict[\"orig\"],\n",
    "            batch_idx,\n",
    "            model,\n",
    "        )\n",
    "        HTML_PLOTS[\"LOGITS_ORIG\"][(batch_idx,)] = str(html_orig)\n",
    "\n",
    "        for (layer, head) in negative_heads:\n",
    "            head_name = f\"{layer}.{head}\"\n",
    "\n",
    "            # Save new log probs (post-ablation)\n",
    "            for ablation_type in [\"mean, direct\", \"zero, direct\", \"mean, patched\", \"zero, patched\"]:\n",
    "\n",
    "                html_ablated = generate_html_for_logit_plot(\n",
    "                    data_toks,\n",
    "                    token_log_probs_dict[f\"{ablation_type}, {layer}.{head}\"],\n",
    "                    token_log_probs_top10_dict[f\"{ablation_type}, {layer}.{head}\"],\n",
    "                    token_log_probs_dict[\"orig\"],\n",
    "                    batch_idx,\n",
    "                    model,\n",
    "                )\n",
    "                HTML_PLOTS[\"LOGITS_ABLATED\"][(batch_idx, head_name, ablation_type)] = str(html_ablated)\n",
    "            \n",
    "            # # Save direct logit effect\n",
    "            dla_neg, dla_pos = generate_html_for_DLA_plot(\n",
    "                data_toks[batch_idx],\n",
    "                direct_effect_log_probs_dict[(layer, head)][batch_idx],\n",
    "                model\n",
    "            )\n",
    "            HTML_PLOTS[\"DLA\"][(batch_idx, head_name, \"neg\")] = str(dla_neg)\n",
    "            HTML_PLOTS[\"DLA\"][(batch_idx, head_name, \"pos\")] = str(dla_pos)\n",
    "    \n",
    "\n",
    "    # ! (5) Calculate the attention probs\n",
    "\n",
    "    for batch_idx in tqdm(range(BATCH_SIZE)):\n",
    "\n",
    "        for layer, head in negative_heads:\n",
    "            head_name = f\"{layer}.{head}\"\n",
    "\n",
    "            # Calculate attention, and info-weighted attention\n",
    "            attn = MODEL_RESULTS.pattern[layer, head][batch_idx]\n",
    "            weighted_attn = einops.einsum(\n",
    "                MODEL_RESULTS.pattern[layer, head][batch_idx],\n",
    "                MODEL_RESULTS.out_norm[layer, head][batch_idx] / MODEL_RESULTS.out_norm[layer, head][batch_idx].max(),\n",
    "                \"seqQ seqK, seqK -> seqQ seqK\"\n",
    "            )\n",
    "\n",
    "            for vis_name, vis_type in {\"Large\": cv.attention.attention_heads, \"Small\": cv.attention.attention_patterns}.items():\n",
    "                html_standard, html_weighted = [\n",
    "                    vis_type(\n",
    "                        attention = x.unsqueeze(0), # (heads=2, seqQ, seqK)\n",
    "                        tokens = data_str_toks_parsed[batch_idx], # list of length seqQ\n",
    "                        attention_head_names = [head_name]\n",
    "                    )\n",
    "                    for x in [attn, weighted_attn]\n",
    "                ]\n",
    "                html_standard, html_weighted = list(map(attn_filter, [html_standard, html_weighted]))\n",
    "                HTML_PLOTS[\"ATTN\"][(batch_idx, head_name, vis_name, \"standard\")] = str(html_standard)\n",
    "                HTML_PLOTS[\"ATTN\"][(batch_idx, head_name, vis_name, \"info-weighted\")] = str(html_weighted)\n",
    "\n",
    "            \n",
    "    # ! (6) Calculate the component of the unembeddings in pre-head residual stream, \n",
    "\n",
    "    str_toks = [model.to_str_tokens(seq) for seq in data_toks]\n",
    "\n",
    "    for layer, head in negative_heads:\n",
    "        head_name = f\"{layer}.{head}\"\n",
    "        # Get the unembedding components in resid_pre just before this head\n",
    "        W_U_comp_avg = MODEL_RESULTS.unembedding_components[layer][\"avg\"]\n",
    "        W_U_comp_top10 = MODEL_RESULTS.unembedding_components[layer][\"top10\"]\n",
    "        # Generate the HTML for these components (separate for including self and excluding self)\n",
    "        html_dict = generate_html_for_unembedding_components_plot(str_toks, W_U_comp_avg[0], W_U_comp_top10[0])\n",
    "        html_rm_self_dict = generate_html_for_unembedding_components_plot(str_toks, W_U_comp_avg[1], W_U_comp_top10[1])\n",
    "        # Add these to dictionary, all at once\n",
    "        HTML_PLOTS[\"UNEMBEDDINGS\"] = {\n",
    "            **HTML_PLOTS[\"UNEMBEDDINGS\"],\n",
    "            # **{(batch_idx, head_name, True): html for batch_idx, html in html_rm_self_dict.items()},\n",
    "            **{(batch_idx, head_name): html for batch_idx, html in html_dict.items()},\n",
    "        }\n",
    "\n",
    "    # Optionally, save the files (we do this if we're generating it from OWT, for the Streamlit page)\n",
    "    if save_files:\n",
    "        with gzip.open(ST_HTML_PATH / \"GZIP_HTML_PLOTS.pkl\", \"wb\") as f:\n",
    "            pickle.dump(HTML_PLOTS, f)\n",
    "\n",
    "    return HTML_PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f generate_4_html_plots generate_4_html_plots(model_results = MODEL_RESULTS, model = model, data_toks = DATA_TOKS, data_str_toks_parsed = DATA_STR_TOKS_PARSED, negative_heads = NEGATIVE_HEADS, save_files = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PLOTS = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS,\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = \"/home/ubuntu/TransformerLens/transformer_lens/rs/callum2/explore_prompts/media/\"\n",
    "# for k, v in HTML_PLOTS.items():\n",
    "#     with open(p + k + \".pkl\", \"wb\") as f:\n",
    "#         pickle.dump(v, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
