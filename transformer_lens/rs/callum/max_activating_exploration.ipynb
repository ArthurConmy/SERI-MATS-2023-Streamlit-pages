{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum.max_activating_exploration import print_best_outputs, find_best_improvements, clear_plots, decompose_attn_scores_full, decompose_attn_scores\n",
    "from transformer_lens.rs.callum.keys_fixed import plot_contribution_to_attn_scores, create_fucking_massive_plot_1, create_fucking_massive_plot_2, project\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "data = get_webtext(seed=6)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDX, HEAD_IDX = (10, 7)\n",
    "W_U = model.W_U.clone()\n",
    "HEAD_HOOK_NAME = utils.get_act_name(\"result\", LAYER_IDX)\n",
    "\n",
    "NUM_PROMPTS = 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def hook_to_ablate_head(head_output: Float[Tensor, \"batch seq_len head_idx d_head\"], hook: HookPoint, head = (LAYER_IDX, HEAD_IDX)):\n",
    "    assert head[0] == hook.layer()\n",
    "    assert \"result\" in hook.name\n",
    "    head_output[:, :, head[1], :] = 0\n",
    "    return head_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max activating examples for 10.7 (by ablation) \n",
    "\n",
    "Want to see where head 10.7 is most useful!\n",
    "\n",
    "We can see cross-entropy loss increases by 0.01 on average when this head is ablated. That might seem like not a lot, but it's actually not far off distribution to other late-stage heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_token_list = []\n",
    "loss_list = []\n",
    "ablated_loss_list = []\n",
    "\n",
    "for i in tqdm(range(NUM_PROMPTS)):\n",
    "    # new_str = data[BATCH_SIZE * i: BATCH_SIZE * (i + 1)]\n",
    "    new_str = data[i]\n",
    "    new_str_tokens = model.to_str_tokens(new_str)\n",
    "    tokens = model.to_tokens(new_str)\n",
    "    # tokens = t.stack(tokens).to(device)\n",
    "    loss = model(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = model.run_with_hooks(tokens, return_type=\"loss\", loss_per_token=True, fwd_hooks=[(HEAD_HOOK_NAME, hook_to_ablate_head)])\n",
    "    loss_list.append(loss)\n",
    "    ablated_loss_list.append(ablated_loss)\n",
    "    str_token_list.append(new_str_tokens)\n",
    "\n",
    "\n",
    "all_loss = t.cat(loss_list, dim=-1).squeeze()\n",
    "all_ablated_loss = t.cat(ablated_loss_list, dim=-1).squeeze()\n",
    "\n",
    "hist(\n",
    "    all_ablated_loss - all_loss,\n",
    "    title=\"Difference in loss after ablating (positive ⇒ loss increases)\",\n",
    "    labels={\"x\": \"Difference in cross-entropy loss\"},\n",
    "    template=\"simple_white\",\n",
    "    add_mean_line=True,\n",
    "    width=1000,\n",
    "    nbins=200,\n",
    "    static=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_tokens = sum(len(i) for i in str_token_list)\n",
    "top_pct = int(total_num_tokens * 0.01)\n",
    "\n",
    "best_k_indices, best_k_loss_decrease = find_best_improvements(str_token_list, loss_list, ablated_loss_list, k=top_pct)\n",
    "worst_k_indices, worst_k_loss_decrease = find_best_improvements(str_token_list, loss_list, ablated_loss_list, k=top_pct, worst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caches_and_tokens = print_best_outputs(\n",
    "    best_k_indices[:3],\n",
    "    best_k_loss_decrease[:3],\n",
    "    hook = (HEAD_HOOK_NAME, hook_to_ablate_head),\n",
    "    model = model,\n",
    "    data = data,\n",
    "    n = 3,\n",
    "    random = False,\n",
    "    return_caches = True,\n",
    "    names_filter = lambda name: name == utils.get_act_name(\"pattern\", LAYER_IDX),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/plots\")\n",
    "\n",
    "clear_plots()\n",
    "\n",
    "window = 100\n",
    "\n",
    "for i, (cache, tokens) in enumerate(caches_and_tokens):\n",
    "    \n",
    "    pattern = cache[\"pattern\", LAYER_IDX][:, HEAD_IDX]\n",
    "    pattern_sliced = pattern[:, -window:, -window:]\n",
    "    html = cv.attention.attention_heads(\n",
    "        attention = pattern_sliced,\n",
    "        tokens = tokens[-window:],\n",
    "        attention_head_names = [f\"{LAYER_IDX}.{HEAD_IDX}, example {i}\"]\n",
    "    )\n",
    "    \n",
    "    with open(str((p / f\"temp_file_{i}.html\").resolve()), \"w\") as f:\n",
    "        f.write(str(html))\n",
    "\n",
    "    print(\"\".join(tokens[-window:]))\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_plots()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max activating examples for 10.7 (by removing direct effect) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_token_list = []\n",
    "loss_list = []\n",
    "patched_loss_list = []\n",
    "\n",
    "for i in tqdm(range(NUM_PROMPTS)):\n",
    "    # new_str = data[BATCH_SIZE * i: BATCH_SIZE * (i + 1)]\n",
    "    new_str = data[i]\n",
    "    new_str_tokens = model.to_str_tokens(new_str)\n",
    "    tokens = model.to_tokens(new_str)\n",
    "    # tokens = t.stack(tokens).to(device)\n",
    "    loss = model(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "    patched_loss = path_patch(\n",
    "        model=model,\n",
    "        orig_input=tokens,\n",
    "        new_cache=\"zero\",\n",
    "        sender_nodes=Node(\"z\", layer=10, head=7),\n",
    "        receiver_nodes=Node(\"resid_post\", layer=11),\n",
    "        direct_includes_mlps=True,\n",
    "        patching_metric=\"loss_per_token\"\n",
    "    )\n",
    "    loss_list.append(loss)\n",
    "    patched_loss_list.append(patched_loss)\n",
    "    str_token_list.append(new_str_tokens)\n",
    "\n",
    "\n",
    "all_loss = t.cat(loss_list, dim=-1).squeeze()\n",
    "all_patched_loss = t.cat(patched_loss_list, dim=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(\n",
    "    all_patched_loss - all_loss,\n",
    "    title=\"Difference in loss after ablating (positive ⇒ loss increases)\",\n",
    "    labels={\"x\": \"Difference in cross-entropy loss\"},\n",
    "    template=\"simple_white\",\n",
    "    add_mean_line=True,\n",
    "    width=800,\n",
    "    nbins=200,\n",
    "    static=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_tokens = sum(len(i) for i in str_token_list)\n",
    "top_pct = int(total_num_tokens * 0.01)\n",
    "top_half_pct = int(total_num_tokens * 0.005)\n",
    "\n",
    "best_k_indices, best_k_loss_decrease = find_best_improvements(str_token_list, loss_list, patched_loss_list, k=top_half_pct)\n",
    "worst_k_indices, worst_k_loss_decrease = find_best_improvements(str_token_list, loss_list, patched_loss_list, k=top_half_pct, worst=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caches_and_tokens = print_best_outputs(\n",
    "    best_k_indices,\n",
    "    best_k_loss_decrease,\n",
    "    hook = (HEAD_HOOK_NAME, hook_to_ablate_head),\n",
    "    model = model,\n",
    "    data = data,\n",
    "    n = 5,\n",
    "    seed = 0,\n",
    "    random = True,\n",
    "    return_caches = False,\n",
    "    # names_filter = lambda name: name == utils.get_act_name(\"pattern\", LAYER_IDX),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All's fair in love and war"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, verify that it does predict \"love\" with some prob. Yes, it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_input = \"All's fair in love and\"\n",
    "answer = \" war\"\n",
    "incorrect = \" love\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(str_input, answer, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to see what the direct effect of head 10.7 is on the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = model.to_tokens(str_input)\n",
    "\n",
    "model.reset_hooks()\n",
    "logits, cache = model.run_with_cache(toks, return_type=\"logits\")\n",
    "logits = logits[0, -1]\n",
    "\n",
    "# resid_post = t.stack([\n",
    "#     cache[\"resid_post\", layer][0, -1]\n",
    "#     for layer in range(model.cfg.n_layers)\n",
    "# ])\n",
    "# resid_post_normalized = resid_post / cache[\"scale\"][0]\n",
    "\n",
    "# logit_lens = einops.einsum(\n",
    "#     resid_post_normalized, model.W_U,\n",
    "#     \"batch d_model, d_model d_vocab -> batch d_vocab\",\n",
    "# )\n",
    "\n",
    "neg_head_output = cache[\"result\", 10][0, -1, 7]\n",
    "neg_head_logits = neg_head_output @ model.W_U\n",
    "assert neg_head_logits.shape == (model.cfg.d_vocab,)\n",
    "neg_head_logprobs = neg_head_logits.log_softmax(dim=-1)\n",
    "\n",
    "top5 = neg_head_logprobs.topk(5, largest=False)\n",
    "\n",
    "for index, value in zip(top5.indices, top5.values):\n",
    "    token = model.to_single_str_token(index.item())\n",
    "    print(f\"|{token}| = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attn = t.concat([\n",
    "    cache[\"pattern\", layer][0] for layer in range(12)\n",
    "])\n",
    "html = cv.attention.attention_heads(\n",
    "    attention=all_attn,\n",
    "    tokens=model.to_str_tokens(PROMPT),\n",
    "    attention_head_names=[f\"{layer}.{head_idx}\" for layer in range(12) for head_idx in range(12)]\n",
    ")\n",
    "f = Path(r\"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/plots\")\n",
    "with open(f / \"temp_file_3.html\", \"w\") as f2:\n",
    "    f2.write(str(html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Does it actually predict \"love\" if head 10.7 isn't firing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_hook(HEAD_HOOK_NAME, hook_to_ablate_head)\n",
    "\n",
    "utils.test_prompt(str_input, answer, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it does! (with second highest probability)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper dive into love and war"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's break down this particular example, and see which heads are responsible for this dumb copying behaviour. I suspect it'll be one of the early ones.\n",
    "\n",
    "Also, I originally thought that maybe nothing copies it (because war and love might be close embeddings). This is possible but unlikely, given love has pretty high logits without any interventions (and `\" War\"` doesn't, nor do any words like this)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct logit attribution - which heads write in the \"love\" direction?\n",
    "\n",
    "I'll look at `\" love\" - \" war\"` because this'll be easier. I'll crib code from the IOI notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS = [\" os\", \" sys\"]\n",
    "# PROMPT = \"\"\"import os\\nimport\"\"\"\n",
    "\n",
    "ANSWERS = [\" love\", \" war\"]\n",
    "PROMPT = \"All's fair in love and\"\n",
    "\n",
    "utils.test_prompt(PROMPT, ANSWERS[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = model.to_tokens(PROMPT)\n",
    "\n",
    "model.reset_hooks()\n",
    "logits, cache = model.run_with_cache(toks, return_type=\"logits\")\n",
    "logits = logits[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens = model.to_tokens(ANSWERS, prepend_bos=False).T\n",
    "\n",
    "answer_residual_directions: Float[Tensor, \"batch=1 2 d_model\"] = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "love_residual_directions, war_residual_directions = answer_residual_directions.unbind(dim=1)\n",
    "logit_diff_directions: Float[Tensor, \"batch=1 d_model\"] = love_residual_directions - war_residual_directions\n",
    "print(f\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    "    logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    ") -> Float[Tensor, \"...\"]:\n",
    "    '''\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "    stack of components in the residual stream.\n",
    "    '''\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "    return einops.einsum(scaled_residual_stack, logit_diff_directions, \"... batch d_model, batch d_model -> ...\") / batch_size\n",
    "\n",
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_residual = einops.rearrange(per_head_residual, \"(layer head) ... -> layer head ...\", layer=model.cfg.n_layers)\n",
    "\n",
    "\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_repeat = residual_stack_to_logit_diff(per_head_residual, cache, logit_diff_directions=love_residual_directions)\n",
    "per_head_logit_correct = residual_stack_to_logit_diff(per_head_residual, cache, logit_diff_directions=war_residual_directions)\n",
    "\n",
    "imshow(\n",
    "    t.stack([per_head_logit_repeat, per_head_logit_correct, per_head_logit_diffs]),\n",
    "    facet_col=0,\n",
    "    facet_labels=[repr(ANSWERS[0]), repr(ANSWERS[1]), repr(ANSWERS[0]) + \" - \" + repr(ANSWERS[1])],\n",
    "    labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "    title=\"Direct Logit Attribution For Each Head\",\n",
    "    width=1200,\n",
    "    # static=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(\n",
    "    [per_head_logit_repeat.sum(-1), per_head_logit_correct.sum(-1)],\n",
    "    names=[\"' love'\", \"' war'\"],\n",
    "    labels={\"index\": \"Layer\", \"value\": \"Logits\", \"variable\": \"Token\"},\n",
    "    title=\"Direct Logit Attribution For Heads In Each Layer\",\n",
    "    height=400,\n",
    "    width=550,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are quite a few heads which are writing `' love'` into the residual stream. `8.6` is most notable, but other important ones are `6.1`, `8.8`, `9.2`, `9.3`, `9.6`.\n",
    "\n",
    "The important thing isn't which heads write to the residual stream though, it's whether `' love'` is present in the residual stream, and if so then by how much (and also what proportion of the `\" love\"` unembedding is used to construct the query?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn_erase_residual_stream_direction(\n",
    "    q_input: Float[Tensor, \"batch seq n_heads d_model\"],\n",
    "    hook: HookPoint,\n",
    "    seq_pos: Union[int, Int[Tensor, \"batch\"]],\n",
    "    direction_to_erase: Float[Tensor, \"d_model\"],\n",
    "    head_idx: int,\n",
    "):\n",
    "    '''\n",
    "    Erases the component of `direction_to_erase` in the `q_input` vector.\n",
    "\n",
    "    seq_pos: the positions we're patching at (for each elem in the batch)\n",
    "    '''\n",
    "    batch, seq_len = q_input.shape[:2]\n",
    "\n",
    "    if isinstance(seq_pos, int):\n",
    "        seq_pos = [seq_pos for _ in range(batch)]\n",
    "    \n",
    "    q_input_slice: Float[Tensor, \"batch d_model\"] = q_input[range(batch), seq_pos, head_idx]\n",
    "    q_input_proj, q_input_perp = project(\n",
    "        q_input_slice, \n",
    "        einops.repeat(direction_to_erase, \"d_model -> batch d_model\", batch=batch)\n",
    "    )\n",
    "\n",
    "    q_input[range(batch), seq_pos, head_idx] = q_input_perp\n",
    "    return q_input\n",
    "\n",
    "\n",
    "NNMH_LIST = [(10, 7), (11, 10)]\n",
    "\n",
    "direction_to_erase = model.W_U.T[model.to_single_token(\" love\")]\n",
    "seq_pos = model.to_str_tokens(PROMPT).index(\" and\")\n",
    "\n",
    "model.reset_hooks()\n",
    "for (layer, head_idx) in NNMH_LIST:\n",
    "    model.add_hook(\n",
    "        name = utils.get_act_name(\"q_input\", layer), \n",
    "        hook = partial(hook_fn_erase_residual_stream_direction, direction_to_erase=direction_to_erase, seq_pos=seq_pos, head_idx=head_idx),\n",
    "    )\n",
    "# model.reset_hooks()\n",
    "\n",
    "utils.test_prompt(PROMPT, ANSWERS[0], model)\n",
    "\n",
    "# logits, cache = model.run_with_cache(\n",
    "#     model.to_tokens(PROMPT),\n",
    "#     names_filter = lambda name: name.endswith(\"pattern\"),\n",
    "# )\n",
    "\n",
    "# cv.attention.attention_patterns(\n",
    "#     attention = t.stack([\n",
    "#         cache[\"pattern\", layer][0, head_idx] for (layer, head_idx) in NNMH_LIST\n",
    "#     ]),\n",
    "#     tokens = model.to_str_tokens(PROMPT),\n",
    "#     attention_head_names = [f\"{layer}.{head_idx}\" for layer, head_idx in NNMH_LIST],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = model.to_tokens(str_input)\n",
    "dest_indices = t.tensor([-1])\n",
    "src_indices = t.tensor([model.to_str_tokens(str_input).index(\" love\")]) # = -2\n",
    "\n",
    "str_input_baseline = \"All's fair in war and\"\n",
    "toks_baseline = model.to_tokens(str_input_baseline)\n",
    "src_baseline_indices = t.tensor([model.to_str_tokens(str_input_baseline).index(\" war\")]) # = -2\n",
    "\n",
    "# attn_scores_full = decompose_attn_scores_full(\n",
    "#     toks = toks,\n",
    "#     dest_indices = dest_indices,\n",
    "#     src_indices = src_indices,\n",
    "#     src_baseline_indices = None,\n",
    "#     nnmh = (10, 7),\n",
    "#     model = model,\n",
    "#     use_effective_embedding = False,\n",
    "#     use_layer0_heads = False\n",
    "# )\n",
    "\n",
    "# create_fucking_massive_plot_1(attn_scores_full)\n",
    "\n",
    "contribution_to_attn_scores = decompose_attn_scores(\n",
    "    toks = toks,\n",
    "    dest_indices = dest_indices,\n",
    "    src_indices = src_indices,\n",
    "    src_baseline_indices = t.tensor([4]), # src_baseline_indices,\n",
    "    toks_baseline = None, # toks_baseline,\n",
    "    nnmh = (10, 7),\n",
    "    model = model,\n",
    "    decompose_by = \"keys\",\n",
    "    intervene_on_query = \"project_to_W_U_IO\",\n",
    "    intervene_on_key = \"project_to_MLP0\",\n",
    "    use_effective_embedding = False,\n",
    "    use_layer0_heads = False,\n",
    ")\n",
    "\n",
    "plot_contribution_to_attn_scores(\n",
    "    t.stack([\n",
    "        contribution_to_attn_scores[('IO_dir', 'MLP0_dir')],\n",
    "        contribution_to_attn_scores[('IO_dir', 'MLP0_perp')],\n",
    "        contribution_to_attn_scores[('IO_perp', 'MLP0_dir')],\n",
    "        contribution_to_attn_scores[('IO_perp', 'MLP0_perp')],\n",
    "    ]),\n",
    "    decompose_by = \"keys\",\n",
    "    facet_labels = [\n",
    "        \"q ∥ W<sub>U</sub>[IO], k ∥ MLP<sub>0</sub>\",\n",
    "        \"q ∥ W<sub>U</sub>[IO], k ⊥ MLP<sub>0</sub>\", \n",
    "        \"q ⊥ W<sub>U</sub>[IO], k ∥ MLP<sub>0</sub>\", \n",
    "        \"q ⊥ W<sub>U</sub>[IO], k ⊥ MLP<sub>0</sub>\"\n",
    "    ],\n",
    "    facet_col_wrap = 2,\n",
    "    title = \"Decompose on query-side, split by projections key & query-side\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = model.to_tokens(str_input)\n",
    "dest_indices = t.tensor([-1])\n",
    "src_indices = t.tensor([model.to_str_tokens(str_input).index(\" love\")]) # = -2\n",
    "\n",
    "str_input_baseline = \"All's fair in war and\"\n",
    "toks_baseline = model.to_tokens(str_input_baseline)\n",
    "src_baseline_indices = t.tensor([model.to_str_tokens(str_input_baseline).index(\" war\")]) # = -2\n",
    "\n",
    "contribution_to_attn_scores = decompose_attn_scores(\n",
    "    toks = toks,\n",
    "    dest_indices = dest_indices,\n",
    "    src_indices = src_indices,\n",
    "    src_baseline_indices = t.tensor([4]), # src_baseline_indices,\n",
    "    toks_baseline = None, # toks_baseline,\n",
    "    nnmh = (10, 7),\n",
    "    model = model,\n",
    "    decompose_by = \"keys\",\n",
    "    intervene_on_query = \"project_to_W_U_IO\",\n",
    "    intervene_on_key = None,\n",
    "    use_effective_embedding = False,\n",
    "    use_layer0_heads = False,\n",
    ")\n",
    "\n",
    "plot_contribution_to_attn_scores(\n",
    "    t.stack([\n",
    "        contribution_to_attn_scores[('IO_dir', 'unchanged')],\n",
    "        contribution_to_attn_scores[('IO_perp', 'unchanged')],\n",
    "    ]),\n",
    "    decompose_by = \"keys\",\n",
    "    facet_labels = [\n",
    "        \"q ∥ W<sub>U</sub>[IO]\",\n",
    "        \"q ⊥ W<sub>U</sub>[IO]\", \n",
    "    ],\n",
    "    facet_col_wrap = 2,\n",
    "    title = \"Decompose on query-side, split by projections key & query-side\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_to_attn_scores.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.log(t.tensor(0.01 * 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_for_copy_suppression(\n",
    "    prompt: str,\n",
    "    answer: str,\n",
    "    repetition: str,\n",
    "    nnmh_list = [(10, 7), (11, 10)],\n",
    "    model = model,\n",
    "    print_test_prompt: bool = False,\n",
    "):\n",
    "    model.reset_hooks()\n",
    "    direction_to_erase = model.W_U.T[model.to_single_token(repetition)]\n",
    "\n",
    "    prompt_str_tokens = model.to_str_tokens(prompt)\n",
    "    # assert prompt_str_tokens[-1] == \" and\"\n",
    "    seq_pos = len(prompt_str_tokens) - 1\n",
    "\n",
    "    if print_test_prompt:\n",
    "        display(HTML(f\"<h2>Forward pass: <span style='color:red'>{prompt} ...</span></h2>\"))\n",
    "        utils.test_prompt(prompt, answer, model, top_k=5, prepend_space_to_answer=False)\n",
    "    else:\n",
    "        logprobs = model(prompt)[0, -1].log_softmax(dim=-1)[model.to_single_token(repetition)]\n",
    "        \n",
    "\n",
    "    for (layer, head_idx) in nnmh_list:\n",
    "        model.add_hook(\n",
    "            name = utils.get_act_name(\"q_input\", layer), \n",
    "            hook = partial(hook_fn_erase_residual_stream_direction, direction_to_erase=direction_to_erase, seq_pos=seq_pos, head_idx=head_idx),\n",
    "        )\n",
    "    \n",
    "    if print_test_prompt:\n",
    "        display(HTML(f\"<h2>Del NNMH projections: <span style='color:red'>{prompt} ...</span></h2>\"))\n",
    "        utils.test_prompt(prompt, answer, model, top_k=5, prepend_space_to_answer=False)\n",
    "    else:\n",
    "        new_logprobs = model(prompt)[0, -1].log_softmax(dim=-1)[model.to_single_token(repetition)]\n",
    "        return new_logprobs - logprobs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"\"\"import os\n",
    "# import\"\"\"\n",
    "# answer = \" sys\"\n",
    "# repetition = \" os\"\n",
    "\n",
    "prompt = \"\"\"from sklearn.model_selection import train_test_split\n",
    "from sklearn.\"\"\"\n",
    "answer = \"linear\"\n",
    "repetition = \"model\"\n",
    "\n",
    "# prompt = \"\"\"import java.util.*;\n",
    "# import java.io.*;\n",
    "# import java.math.*;\n",
    "# import java.\"\"\"\n",
    "# answer = \"time\"\n",
    "# repetition = \" math\"\n",
    "\n",
    "test_prompt_for_copy_suppression(\n",
    "    prompt = \"\"\"All's fair in love and\"\"\",\n",
    "    answer = \" war\",\n",
    "    repetition = \" love\",\n",
    "    print_test_prompt = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_proj_outputs(\n",
    "    cache: ActivationCache,\n",
    "    seq_pos: Union[int, Int[Tensor, \"batch seq\"]],\n",
    "    head_list: List[Tuple[int, int]],\n",
    "    direction_to_erase: Float[Tensor, \"d_model\"],\n",
    "):\n",
    "    all_results_proj = []\n",
    "\n",
    "    batch = cache[\"result\", 0].shape[0]\n",
    "    if isinstance(seq_pos, int):\n",
    "        seq_pos = [seq_pos for _ in range(batch)]\n",
    "    \n",
    "    for head in head_list:\n",
    "        layer, head_idx = head\n",
    "        result = cache[\"result\", layer][range(batch), seq_pos, head_idx]\n",
    "    \n",
    "        result_proj, result_perp = project(\n",
    "            result, \n",
    "            einops.repeat(direction_to_erase, \"d_model -> batch d_model\", batch=batch)\n",
    "        )\n",
    "\n",
    "        all_results_proj.append(result_proj)\n",
    "    \n",
    "    return sum(all_results_proj)\n",
    "\n",
    "\n",
    "def test_prompt_for_copy_suppression_big_brained(\n",
    "    prompt: str,\n",
    "    answer: str,\n",
    "    repetition: str,\n",
    "    copy_heads_list: List[Tuple[int, int]],\n",
    "    nnmh_list = [(10, 7), (11, 10)],\n",
    "    model = model,\n",
    "    print_test_prompt: bool = False,\n",
    "):\n",
    "    model.reset_hooks()\n",
    "    direction_to_erase = model.W_U.T[model.to_single_token(repetition)]\n",
    "\n",
    "    prompt_str_tokens = model.to_str_tokens(prompt)\n",
    "    # assert prompt_str_tokens[-1] == \" and\"\n",
    "    seq_pos = len(prompt_str_tokens) - 1\n",
    "\n",
    "    logits, cache = model.run_with_cache(prompt, names_filter=lambda name: name.endswith(\"result\"))\n",
    "    if print_test_prompt:\n",
    "        display(HTML(f\"<h2>Forward pass: <span style='color:red'>{prompt} ...</span></h2>\"))\n",
    "        utils.test_prompt(prompt, answer, model, top_k=5, prepend_space_to_answer=False)\n",
    "    else:\n",
    "        logprobs = logits[0, -1].log_softmax(dim=-1)[model.to_single_token(repetition)]\n",
    "    \n",
    "    all_results_perp = get_head_proj_outputs(cache, seq_pos, head_list=copy_heads_list, direction_to_erase=direction_to_erase)\n",
    "\n",
    "    for (layer, head_idx) in nnmh_list:\n",
    "        model.add_hook(\n",
    "            name = utils.get_act_name(\"q_input\", layer), \n",
    "            hook = lambda q_input, hook: q_input - all_results_perp,\n",
    "        )\n",
    "    \n",
    "    if print_test_prompt:\n",
    "        display(HTML(f\"<h2>Del NNMH projections: <span style='color:red'>{prompt} ...</span></h2>\"))\n",
    "        utils.test_prompt(prompt, answer, model, top_k=5, prepend_space_to_answer=False)\n",
    "    else:\n",
    "        new_logprobs = model(prompt)[0, -1].log_softmax(dim=-1)[model.to_single_token(repetition)]\n",
    "        return new_logprobs - logprobs\n",
    "\n",
    "\n",
    "INDUCTION_HEADS = [(5, 1), (6, 9), (7, 2), (7, 10), (9, 6), (9, 9)]\n",
    "EARLY_INDUCTION_HEADS = [(5, 1), (6, 9), (7, 2), (7, 10)]\n",
    "EARLY_HEADS = [(layer, head) for layer in range(10) for head in range(12)]\n",
    "\n",
    "# prompt = \"\"\"import os\n",
    "# import\"\"\"\n",
    "# answer = \" sys\"\n",
    "# repetition = \" os\"\n",
    "\n",
    "# prompt = \"\"\"Behind the first door is a car.\n",
    "# Behind the second door is a goat.\n",
    "# Behind the third\"\"\" # and final door / and fourth doors\n",
    "# answer = \" and\"\n",
    "# repetition = \" door\"\n",
    "\n",
    "prompt = \"\"\"from sklearn.model_selection import train_test_split\n",
    "from sklearn.\"\"\"\n",
    "answer = \"linear\"\n",
    "repetition = \"model\"\n",
    "\n",
    "# prompt = \"\"\"library(ggplot2)\n",
    "# library(\"\"\"\n",
    "# answer = \"d\"\n",
    "# repetition = \"gg\"\n",
    "\n",
    "# prompt = \"\"\"import java.util.*;\n",
    "# import java.io.*;\n",
    "# import java.math.*;\n",
    "# import java.\"\"\"\n",
    "# answer = \"time\"\n",
    "# repetition = \" math\"\n",
    "\n",
    "test_prompt_for_copy_suppression_big_brained(\n",
    "    prompt = prompt,\n",
    "    answer = answer,\n",
    "    repetition = repetition,\n",
    "    print_test_prompt = True,\n",
    "    copy_heads_list = EARLY_INDUCTION_HEADS, # EARLY_HEADS, # INDUCTION_HEADS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"pattern\", 10][0, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"from sklearn.model_selection import train_test_split\n",
    "from sklearn.\"\"\"\n",
    "answer = \"linear\"\n",
    "repetition = \"model\"\n",
    "\n",
    "nnmh_list = [(10, 7), (11, 10)]\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "\n",
    "direction_to_erase = model.W_U.T[model.to_single_token(repetition)]\n",
    "prompt_str_tokens = model.to_str_tokens(prompt)\n",
    "batch = 1\n",
    "seq_pos = len(prompt_str_tokens) - 1\n",
    "all_results_perp = get_head_proj_outputs(cache, seq_pos, head_list=EARLY_HEADS, direction_to_erase=direction_to_erase)\n",
    "\n",
    "def my_hook(q_input, hook, head_idx):\n",
    "    q_input[range(batch), seq_pos, head_idx] -= all_results_perp\n",
    "    return q_input\n",
    "\n",
    "for (layer, head_idx) in nnmh_list:\n",
    "    # model.add_hook(\n",
    "    #     name = utils.get_act_name(\"q_input\", layer), \n",
    "    #     hook = partial(my_hook, head_idx=head_idx),\n",
    "    #     is_permanent = True,\n",
    "    # )\n",
    "    model.add_hook(\n",
    "        name = utils.get_act_name(\"q_input\", layer), \n",
    "        hook = partial(hook_fn_erase_residual_stream_direction, direction_to_erase=direction_to_erase, seq_pos=seq_pos, head_idx=head_idx),\n",
    "    )\n",
    "\n",
    "logits_patched, cache_patched = model.run_with_cache(prompt)\n",
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "for L, H in nnmh_list:\n",
    "    display(cv.attention.attention_patterns(\n",
    "        tokens = model.to_str_tokens(prompt),\n",
    "        attention = t.stack([cache[\"pattern\", L][0, H], cache_patched[\"pattern\", L][0, H], t.clamp(cache[\"pattern\", L][0, H] - cache_patched[\"pattern\", L][0, H], 0, 1)]),\n",
    "        attention_head_names = [f\"{L}.{H} clean\", f\"{L}.{H} patched\", \"Diff\"],\n",
    "    ))\n",
    "\n",
    "    print(cache[\"pattern\", L][0, H][-1, 5].item())\n",
    "    print(cache_patched[\"pattern\", L][0, H][-1, 5].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r\"\"\"We remember him for his kindness and generosity\n",
    "It's a balance of tradition and innovation\n",
    "She loved the colors of pink and blue\n",
    "The main ingredients are flour and sugar\n",
    "They chose to adopt a lifestyle of simplicity and minimalism\n",
    "Their marriage was a combination of trust and respect\n",
    "The garden is full of birds and butterflies\n",
    "This meal needs a touch of salt and pepper\n",
    "The sunset was a blend of orange and purple\n",
    "The store sells goods new and used\n",
    "The climate here is both hot and humid\n",
    "The area is known for its wine and cheese\n",
    "The theme of the party was black and white\n",
    "She's a blend of beauty and brains\n",
    "This painting is a fusion of reality and fantasy\n",
    "His life was a mixture of pleasure and pain\n",
    "The landscape was filled with trees and flowers\n",
    "She was a paradox of innocence and cunning\n",
    "The play was a mix of tragedy and comedy\n",
    "The book is a combination of facts and fiction\n",
    "The sky was filled with stars and moonlight\n",
    "We traveled by both land and sea\n",
    "The pie was a combination of sweet and savory\n",
    "This role requires both skill and dedication\n",
    "The holiday was filled with rest and relaxation\n",
    "Their home was full of love and warmth\n",
    "The music was a blend of rhythm and melody\n",
    "We saw a combination of elephants and lions\n",
    "The path was full of twists and turns\n",
    "Life is a puzzle of choices and consequences\n",
    "His voice carried a tone of authority and confidence\n",
    "The project demands a blend of creativity and precision\n",
    "The dance performance was a mix of grace and energy\n",
    "They navigated through challenges with resilience and determination\n",
    "The painting depicted a harmony of light and shadow\n",
    "The city is a melting pot of cultures and traditions\n",
    "The movie captured a balance of laughter and tears\n",
    "Their friendship was built on trust and loyalty\n",
    "The dish had a combination of spicy and sweet flavors\n",
    "The competition showcased a variety of skills and talents\n",
    "The conversation flowed with ease and depth\n",
    "The dress was a fusion of vintage and modern styles\n",
    "The team displayed unity and teamwork on the field\n",
    "The conversation revealed a mix of excitement and nervousness\n",
    "The decision required a blend of logic and intuition\n",
    "The exhibition featured a collection of paintings and sculptures\n",
    "The journey offered a mix of challenges and rewards\n",
    "The recipe called for a combination of herbs and spices\n",
    "The story unfolded with twists and surprises\n",
    "The meeting covered topics of importance and urgency\n",
    "The performance was a fusion of music and dance\n",
    "The ceremony celebrated love and commitment\n",
    "The speech was a balance of humor and sincerity\n",
    "The dish was a medley of flavors and textures\n",
    "The puzzle contained a mixture of easy and difficult clues\n",
    "The forest was filled with sounds of birds and animals\n",
    "The dessert had a combination of sweetness and tanginess\n",
    "The workshop explored the intersection of science and art\n",
    "The concert showcased a blend of classical and contemporary\n",
    "The presentation included a mix of visuals and data\n",
    "The cityscape was a tapestry of lights and buildings\n",
    "The relationship flourished with open communication and trust\n",
    "The event was a fusion of fashion and culture\n",
    "The hike offered a blend of breathtaking views and challenging\n",
    "The negotiations required a combination of compromise and assertiveness\n",
    "The artwork depicted a contrast of light and darkness\n",
    "The conversation revealed a mix of emotions and perspectives\n",
    "The performance blended storytelling and dance\n",
    "The garden featured a variety of flowers and plants\n",
    "The song was a harmony of voices and instruments\n",
    "The recipe combined fresh ingredients and bold\n",
    "The discussion explored different angles and possibilities\n",
    "The room was filled with laughter and conversation\n",
    "The landscape was a mix of mountains and valleys\n",
    "The dance routine incorporated elements of strength and grace\n",
    "The project combined research and creativity\n",
    "The play evoked a range of emotions and reactions\n",
    "The dish had a balance of sweetness and acidity\n",
    "The exhibit showcased a mixture of paintings and sculptures\n",
    "The relationship was a combination of love and friendship\n",
    "The journey offered both challenges and rewards\n",
    "The novel intertwined multiple storylines and perspectives\n",
    "The presentation included a blend of visuals and statistics\n",
    "The atmosphere was filled with energy and excitement\n",
    "The dessert had a fusion of flavors and textures\n",
    "The team demonstrated a mix of strategy and skill\n",
    "The workshop explored the connection between art and technology\n",
    "The movie was a medley of action and drama\n",
    "The meeting covered a combination of topics and issues\n",
    "The city was a blend of old and new\n",
    "The forest was alive with the sounds of animals and birds\n",
    "The dish had a combination of spicy and tangy\n",
    "The class explored the intersection of science and philosophy\n",
    "The performance combined dance and theater\n",
    "The presentation included a mix of anecdotes and data\n",
    "The relationship thrived on understanding and compromise\n",
    "The event was a fusion of music and art\n",
    "The hike provided a mix of breathtaking views and challenging\n",
    "The artwork portrayed a juxtaposition of colors and shapes\n",
    "The conversation revealed a blend of opinions and perspectives\n",
    "The concert featured a fusion of jazz and rock music\n",
    "The workshop delved into the synthesis of psychology and mindfulness\n",
    "The dish consisted of a medley of vegetables and herbs\n",
    "The speech delivered a combination of inspiration and motivation\n",
    "The garden displayed a variety of fragrant flowers and greenery\n",
    "The project required a balance of innovation and practicality\n",
    "The play incorporated a mix of comedy and tragedy\n",
    "The landscape offered a juxtaposition of serene lakes and mountains\n",
    "The song blended different genres and musical styles\n",
    "The class explored the fusion of literature and psychology\n",
    "The meeting covered a variety of topics ranging from finance to sustainability\n",
    "The city was a melting pot of diverse cultures and traditions\n",
    "The forest was a mix of colorful trees and wildflowers\n",
    "The dish had a harmony of spicy and savory\n",
    "The workshop delved into the overlap of technology and education\n",
    "The movie was a combination of suspense and romance\n",
    "The presentation included a mix of real-life examples and theory\n",
    "The relationship was a mixture of passion and stability\n",
    "The event was a fusion of sports and entertainment\n",
    "The conversation revealed a combination of curiosity and knowledge\n",
    "The performance blended music and poetry\n",
    "The discussion explored the intersection of politics and ethics\n",
    "The exhibit showcased a mixture of sculptures and installations\n",
    "The play evoked a range of emotions, from laughter to tears\n",
    "The dance routine incorporated elements of traditional and modern\n",
    "The song had a harmony of melodic vocals and rhythmic\n",
    "The class explored the connection between biology and psychology\n",
    "The meeting covered a combination of challenges and opportunities\n",
    "The workshop explored the integration of art and sustainability\"\"\"\n",
    "\n",
    "sentences = s.split(\"\\n\")\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for prompt in sentences:\n",
    "    prompt_split = prompt.split(\" \")\n",
    "    answer = \" \" + prompt_split[-1]\n",
    "    repetition = \" \" + prompt_split[-3]\n",
    "    prompt = \" \".join(prompt_split[:-1])\n",
    "    inputs.append((prompt, answer, repetition))\n",
    "\n",
    "logprobs_diff = []\n",
    "\n",
    "for prompt, answer, repetition in inputs:\n",
    "    logprobs_diff.append(\n",
    "        test_prompt_for_copy_suppression(\n",
    "            prompt = prompt,\n",
    "            answer = answer,\n",
    "            repetition = repetition,\n",
    "            print_test_prompt = False,\n",
    "        ).item()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(logprobs_diff, width=700, template=\"simple_white\", static=True, nbins=40, title=\"Naive Copy Suppression: logit difference from deleting unembedding of<br>earlier token from 10.7 & 11.10 query input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"One man's trash is another man's\"\n",
    "answer = \" treasure\"\n",
    "repetition = \" trash\"\n",
    "\n",
    "test_prompt_for_copy_suppression(\n",
    "    prompt = prompt,\n",
    "    answer = answer,\n",
    "    repetition = repetition,\n",
    "    print_test_prompt = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"import os\n",
    "# import\"\"\"\n",
    "# answer = \" sys\"\n",
    "# repetition = \" os\"\n",
    "\n",
    "prompt = \"\"\"from sklearn.model_selection import train_test_split\n",
    "from sklearn.\"\"\"\n",
    "answer = \"linear\"\n",
    "repetition = \"model\"\n",
    "\n",
    "prompt = \"\"\"All's fair in love and\"\"\"\n",
    "answer = \" war\"\n",
    "repetition = \"love\"\n",
    "\n",
    "# prompt = \"\"\"import java.util.*;\n",
    "# import java.io.*;\n",
    "# import java.math.*;\n",
    "# import java.\"\"\"\n",
    "# answer = \"time\"\n",
    "# repetition = \" math\"\n",
    "\n",
    "test_prompt_for_copy_suppression(\n",
    "    prompt = prompt,\n",
    "    answer = answer,\n",
    "    repetition = repetition,\n",
    "    print_test_prompt = True,\n",
    ")\n",
    "\n",
    "# prompt = \"\"\"wOIEICALUIaoi apsmdu q wd qwdl po aslnk csa la wOIEICALUIaoi apsmdu q wd qwdl po aslnk csa\"\"\"\n",
    "# answer = \" la\"\n",
    "# repetition = \" la\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = model.to_tokens(prompt)\n",
    "\n",
    "model.reset_hooks()\n",
    "logits, cache = model.run_with_cache(toks, return_type=\"logits\")\n",
    "logits = logits[0, -1]\n",
    "\n",
    "answer_tokens = model.to_tokens([repetition, answer], prepend_bos=False).T\n",
    "\n",
    "answer_residual_directions: Float[Tensor, \"batch=1 2 d_model\"] = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "love_residual_directions, war_residual_directions = answer_residual_directions.unbind(dim=1)\n",
    "logit_diff_directions: Float[Tensor, \"batch=1 d_model\"] = love_residual_directions - war_residual_directions\n",
    "print(f\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    "    logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    ") -> Float[Tensor, \"...\"]:\n",
    "    '''\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "    stack of components in the residual stream.\n",
    "    '''\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "    return einops.einsum(scaled_residual_stack, logit_diff_directions, \"... batch d_model, batch d_model -> ...\") / batch_size\n",
    "\n",
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_residual = einops.rearrange(per_head_residual, \"(layer head) ... -> layer head ...\", layer=model.cfg.n_layers)\n",
    "\n",
    "\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_repeat = residual_stack_to_logit_diff(per_head_residual, cache, logit_diff_directions=love_residual_directions)\n",
    "per_head_logit_correct = residual_stack_to_logit_diff(per_head_residual, cache, logit_diff_directions=war_residual_directions)\n",
    "\n",
    "imshow(\n",
    "    t.stack([per_head_logit_repeat, per_head_logit_correct, per_head_logit_diffs]),\n",
    "    facet_col=0,\n",
    "    facet_labels=[repr(repetition), repr(answer), repr(repetition) + \" - \" + repr(answer)],\n",
    "    facet_label_size=16,\n",
    "    labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "    title=\"Direct Logit Attribution For Each Head\",\n",
    "    width=1200,\n",
    "    # static=True,\n",
    ")\n",
    "imshow(\n",
    "    per_head_logit_repeat,\n",
    "    facet_label_size=16,\n",
    "    labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "    title='Direct Logit Attribution For Each Head, for <span style=\"font-family: \\'Lucida Console\\'; color; \\'red\\'\">\"model\"</span>',\n",
    "    width=700,\n",
    "    # static=True,\n",
    "    # text_auto=\".2f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IH = [(7, 2), (7, 10), (9, 6)]\n",
    "\n",
    "cv.attention.attention_patterns(\n",
    "    tokens = model.to_str_tokens(prompt),\n",
    "    attention = t.stack([cache[\"pattern\", L][0, H] for L, H in IH]),\n",
    "    attention_head_names = [f\"{L}.{H}\" for L, H in IH],\n",
    ")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
