{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate AI vs CS plots\n",
    "\n",
    "Open question - how much of anti-induction is actually just copy-suppression?\n",
    "\n",
    "We're answering this question by doing a large scatter plot of two metrics.\n",
    "\n",
    "On the x-axis is **copy-suppression scores on the IOI distribution**. This is calculated as follows:\n",
    "\n",
    "* For the given attention head, take the result vector being moved from the IO token to the end token.\n",
    "* Measure its direct logit attribution in the direction of the IO token.\n",
    "\n",
    "This should be very positive for copy heads, and very negative for our negative heads.\n",
    "\n",
    "On the y-axis is **anti-induction scores on the IOI distribution**. This is calculated as follows:\n",
    "\n",
    "* Input a random repeating sequence (i.e. the model's BOS token, followed by 2 copies of the same random sequence concatenated together).\n",
    "* Measure the model's direct logit attribution on the correct token.\n",
    "\n",
    "This should be positive for induction heads, and negative for anti-induction heads.\n",
    "\n",
    "## How could this be improved?\n",
    "\n",
    "The anti-induction metric is pretty clear and obvious. I'm not quite as happy with the IOI metric, because this is just one of the many cases where negative behaviour is displayed. However, it seems like a pretty crisp example.\n",
    "\n",
    "Other possible ideas:\n",
    "\n",
    "* Run the model on OWT (but this might take a long time!).\n",
    "* Take classic sentences, like the \"breaking the pattern\" example about picking up \"the third and final box\".\n",
    "    * However, an issue with this is that this is kinda anti-induction.\n",
    "* Measure directly from the weights - some combination of \"average log self-attention rank\" and \"average log self-suppression rank\" for non-function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/TransformerLens/transformer_lens/__init__.py:4: UserWarning: Doing ACCELERATE_DISABLE_RICH ...\n",
      "  warnings.warn(\"Doing ACCELERATE_DISABLE_RICH ...\")\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2023-07-25 17:11:02.575444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 17:11:02.707179: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            155-248-204-156\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4126\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           155-248-204-156\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/utils/imports.py:178: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/TransformerLens/transformer_lens/cautils/notebook.py:11: UserWarning: Running load_ext autoreload...\n",
      "  warnings.warn(\"Running load_ext autoreload...\")\n",
      "/home/ubuntu/TransformerLens/transformer_lens/cautils/utils.py:10: UserWarning: Setting grad enabled false...\n",
      "  warnings.warn(\"Setting grad enabled false...\")\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "from transformer_lens.rs.callum2.explore_prompts.explore_prompts_utils import (\n",
    "    create_title_and_subtitles,\n",
    "    parse_str,\n",
    "    parse_str_tok_for_printing,\n",
    "    parse_str_toks_for_printing,\n",
    "    topk_of_Nd_tensor,\n",
    "    ST_HTML_PATH,\n",
    ")\n",
    "\n",
    "from transformer_lens.rs.callum2.explore_prompts.model_results_3 import (\n",
    "    FUNCTION_STR_TOKS,\n",
    "    project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "gpt2.set_use_attn_result(False)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_suppression_scores_ioi(model: HookedTransformer, N: int):\n",
    "\n",
    "    all_results = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device, dtype=t.float)\n",
    "\n",
    "    ioi_dataset, ioi_cache = generate_data_and_caches(\n",
    "        N,\n",
    "        model,\n",
    "        seed=42,\n",
    "        prepend_bos=True,\n",
    "        only_ioi=True,\n",
    "        symmetric=True,\n",
    "        names_filter=lambda name: any([name.endswith(x) for x in [\"scale\", \"v\", \"pattern\"]])\n",
    "    )\n",
    "\n",
    "    io_unembeddings = model.W_U.T[ioi_dataset.io_tokenIDs] # (batch, d_model)\n",
    "\n",
    "    scale = ioi_cache[\"scale\"] # (batch, seq, 1)\n",
    "    scale = scale[range(N), ioi_dataset.word_idx[\"end\"]] # (batch, 1)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        v = ioi_cache[\"v\", layer] # (batch, seq, n_heads, d_head)\n",
    "\n",
    "        v_io = v[range(N), ioi_dataset.word_idx[\"IO\"]] # (batch, n_heads, d_head)\n",
    "\n",
    "        # Get result (before attn patterns)\n",
    "        result_io = einops.einsum(\n",
    "            v_io, model.W_O[layer],\n",
    "            \"batch n_heads d_head, n_heads d_head d_model -> batch n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Get result moved to `end` token (after attn patterns)\n",
    "        patterns = ioi_cache[\"pattern\", layer] # (batch, n_heads, seqQ, seqK)\n",
    "        patterns_end_to_io = patterns[range(N), :, ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"IO\"]] # (batch, n_heads)\n",
    "        result_io_to_end = einops.einsum(\n",
    "            result_io, patterns_end_to_io,\n",
    "            \"batch n_heads d_model, batch n_heads -> batch n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Finally, get attribution (which includes effect of layernorm)\n",
    "        dla = einops.einsum(\n",
    "            result_io_to_end, io_unembeddings,\n",
    "            \"batch n_heads d_model, batch d_model -> batch n_heads\"\n",
    "        ) / scale\n",
    "        dla = einops.reduce(dla, \"batch n_heads -> n_heads\", \"mean\")\n",
    "        \n",
    "        all_results[layer] = dla\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anti_induction_scores(model: HookedTransformer, N: int, seq_len: int = 30):\n",
    "\n",
    "    tokens_to_repeat = t.randint(0, model.cfg.d_vocab, (N, seq_len), device=device)\n",
    "    bos_tokens = t.full((N, 1), model.tokenizer.bos_token_id, device=device, dtype=t.long)\n",
    "    tokens = t.concat([bos_tokens, tokens_to_repeat, tokens_to_repeat], dim=1)\n",
    "    assert tokens.shape == (N, 2*seq_len+1)\n",
    "        \n",
    "    all_results = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device, dtype=t.float)\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        tokens,\n",
    "        return_type = None,\n",
    "        names_filter = lambda name: any([name.endswith(x) for x in [\"scale\", \"v\", \"pattern\"]])\n",
    "    )\n",
    "\n",
    "    rep_unembeddings = model.W_U.T[tokens_to_repeat[:, 1:]] # (batch, rep_seq_pos, d_model)\n",
    "\n",
    "    batch_indices = einops.repeat(t.arange(N, device=device), \"batch -> batch seq\", seq=seq_len-1)\n",
    "    dest_indices = einops.repeat(t.arange(seq_len+1, 2*seq_len, device=device), \"seq -> batch seq\", batch=N)\n",
    "    src_indices = einops.repeat(t.arange(2, seq_len+1, device=device), \"seq -> batch seq\", batch=N)\n",
    "\n",
    "    scale = cache[\"scale\"] # (batch, seq, 1)\n",
    "    scale = scale[batch_indices, dest_indices] # (batch, rep_seq_pos, 1)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        v = cache[\"v\", layer] # (batch, seq, n_heads, d_head)\n",
    "\n",
    "        v_io = v[batch_indices, src_indices] # (batch, rep_seq_pos, n_heads, d_head)\n",
    "\n",
    "        # Get result (before attn patterns)\n",
    "        result_io = einops.einsum(\n",
    "            v_io, model.W_O[layer],\n",
    "            \"batch rep_seq_pos n_heads d_head, n_heads d_head d_model -> batch rep_seq_pos n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Get result moved to dest tokens (after attn patterns)\n",
    "        patterns = cache[\"pattern\", layer] # (batch, n_heads, seqQ, seqK)\n",
    "        patterns_end_to_io = patterns[batch_indices, :, dest_indices, src_indices] # (batch, rep_seq_pos, n_heads)\n",
    "        result_io_to_end = einops.einsum(\n",
    "            result_io, patterns_end_to_io,\n",
    "            \"batch rep_seq_pos n_heads d_model, batch rep_seq_pos n_heads -> batch rep_seq_pos n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Finally, get attribution (which includes effect of layernorm)\n",
    "        dla = einops.einsum(\n",
    "            result_io_to_end, rep_unembeddings,\n",
    "            \"batch rep_seq_pos n_heads d_model, batch rep_seq_pos d_model -> batch rep_seq_pos n_heads\"\n",
    "        ) / scale\n",
    "        dla = einops.reduce(dla, \"batch rep_seq_pos n_heads -> n_heads\", \"mean\")\n",
    "        \n",
    "        all_results[layer] = dla\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix\n",
    "\n",
    "\n",
    "def get_effective_embedding(model: HookedTransformer, only_mlps: bool = False) -> Float[Tensor, \"d_vocab d_model\"]:\n",
    "\n",
    "    resid = model.W_E.unsqueeze(0)\n",
    "\n",
    "    pre_attention = model.blocks[0].ln1(resid)\n",
    "    attn_out = einops.einsum(\n",
    "        pre_attention, \n",
    "        model.W_V[0],\n",
    "        model.W_O[0],\n",
    "        \"b s d_model, num_heads d_model d_head, num_heads d_head d_model_out -> b s d_model_out\",\n",
    "    )\n",
    "    resid_mid = attn_out + resid\n",
    "    normalized_resid_mid = model.blocks[0].ln2(resid_mid)\n",
    "    mlp_out = model.blocks[0].mlp(normalized_resid_mid)\n",
    "    resid = resid_mid + mlp_out\n",
    "\n",
    "    if only_mlps:\n",
    "        W_EE0 = mlp_out.squeeze()\n",
    "        return W_EE0\n",
    "    else:\n",
    "        W_EE = resid.squeeze()\n",
    "        return W_EE\n",
    "\n",
    "\n",
    "def get_weight_scores(model: HookedTransformer, N: int = 100, tied_embeddings: bool = False):\n",
    "    '''\n",
    "    Returns the mean log(rank+1) (from sampling) of two things:\n",
    "\n",
    "        OV circuit metric: how much does a word suppress itself?\n",
    "        QK circuit metric: how much does a word attend to itself?\n",
    "\n",
    "    For instance, with head 10.7, most of the values will be 1 (or very small) because words suppress\n",
    "    themselves & attend to themselves. So average log rank will be very close to zero. But for copying\n",
    "    heads, most of the values will be pretty large, so the average log rank will be much larger.\n",
    "\n",
    "    Since log has a much higher gradient at smaller values, this will single out super negative heads.\n",
    "    Heads which are \"middle neg/pos\" or \"highly positive\" won't be very distinguishable.\n",
    "\n",
    "    The result has shape (3, n_layers, n_heads), where:\n",
    "        [0] -> OV scores (filtered for neg head results)\n",
    "        [1] -> QK scores (filtered for neg head results)\n",
    "        [2] -> The baseline-subtracted product\n",
    "    '''\n",
    "    W_U = model.W_U\n",
    "    W_E = model.W_E if not(tied_embeddings) else get_effective_embedding(model, only_mlps=True)\n",
    "\n",
    "    results = t.zeros((2, model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "\n",
    "            full_OV_matrix = FactoredMatrix(\n",
    "                W_E @ model.W_V[layer][head],\n",
    "                model.W_O[layer][head] @ W_U\n",
    "            )\n",
    "            full_QK_matrix = FactoredMatrix(\n",
    "                W_E @ model.W_K[layer][head],\n",
    "                model.W_Q[layer][head].T @ W_U\n",
    "            )\n",
    "\n",
    "            # Are the diagonal values the most negative (i.e. self-suppression)?\n",
    "            random_indices = t.randint(0, model.cfg.d_vocab, (N,), device=device)\n",
    "            OV_slice = full_OV_matrix.A[random_indices, :] @ full_OV_matrix.B\n",
    "            OV_diag_values = OV_slice[range(N), random_indices].unsqueeze(1)\n",
    "            OV_ranks = (OV_diag_values > OV_slice).sum(dim=1).float()\n",
    "            OV_avg_log_rank = t.log(OV_ranks + 1).mean().item()\n",
    "\n",
    "            # Are the diagonal values the most positive (i.e. prediction-attention)?\n",
    "            random_indices = t.randint(0, model.cfg.d_vocab, (N,), device=device)\n",
    "            QK_slice = full_QK_matrix.A @ full_QK_matrix.B[:, random_indices]\n",
    "            QK_diag_values = QK_slice[random_indices, range(N)].unsqueeze(0)\n",
    "            QK_ranks = (QK_diag_values < QK_slice).sum(dim=0).float()\n",
    "            QK_avg_log_rank = t.log(QK_ranks + 1).mean().item()\n",
    "\n",
    "            results[0, layer, head] = OV_avg_log_rank\n",
    "            results[1, layer, head] = QK_avg_log_rank\n",
    "\n",
    "    zero_point = t.tensor(gpt2.cfg.d_vocab).log().item() - 1\n",
    "\n",
    "    results = t.stack([\n",
    "        results[0] - zero_point,\n",
    "        results[1] - zero_point,\n",
    "        (results[0] - zero_point) * (results[1] - zero_point),\n",
    "    ])\n",
    "    results[:2] *= (results[:2] < 0)\n",
    "    results[2] *= -1 * (results[0] < 0) * (results[1] < 0)\n",
    "    results[2] *= (results[:2].abs().max() / results[2].abs().max())\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40 # 91 for scatter, 51 for viz\n",
    "SEQ_LEN = 50 # 100 for scatter, 61 for viz (no more, cause attn)\n",
    "\n",
    "def process_webtext_1(\n",
    "    seed: int = 6,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    DATA_STR = get_webtext(seed=seed)[:batch_size]\n",
    "    DATA_STR = [parse_str(s) for s in DATA_STR]\n",
    "    clear_output()\n",
    "    return DATA_STR\n",
    "\n",
    "def process_webtext_2(\n",
    "    model: HookedTransformer,\n",
    "    DATA_STR: List[str],\n",
    "    seq_len: int = SEQ_LEN,\n",
    "    verbose: bool = False,\n",
    ") -> Int[Tensor, \"batch seq\"]:\n",
    "    DATA_TOKS = model.to_tokens(DATA_STR)\n",
    "\n",
    "    if seq_len < 1024:\n",
    "        DATA_TOKS = DATA_TOKS[:, :seq_len]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Shape = {DATA_TOKS.shape}\\n\")\n",
    "        # DATA_STR_TOKS = model.to_str_tokens(DATA_STR)\n",
    "        # DATA_STR_TOKS = [str_toks[:seq_len] for str_toks in DATA_STR_TOKS]\n",
    "        # DATA_STR_TOKS_PARSED = list(map(parse_str_toks_for_printing, DATA_STR_TOKS))\n",
    "        # print(\"First prompt:\\n\" + \"\".join(DATA_STR_TOKS[0]))\n",
    "\n",
    "    return DATA_TOKS.to(device)\n",
    "\n",
    "\n",
    "DATA_STR = process_webtext_1()\n",
    "# DATA_TOKS = process_webtext_2(gpt2, DATA_STR)\n",
    "# BATCH_SIZE, SEQ_LEN = DATA_TOKS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_vivo_copy_suppression_scores(model: HookedTransformer, DATA_STR: Int[Tensor, \"batch seq\"]):\n",
    "    '''\n",
    "    This is the current preferred way of getting copy suppression scores for an attention head. The methodology is as follows:\n",
    "\n",
    "    For each destination token, we...\n",
    "        1. Find the source token S s.t. the unembedding of S is most present in the residual stream (i.e. logit lens), excluding any S = function words\n",
    "        2. Take the result vector from that source token (multiplied by its attention probability)\n",
    "        3. Project it onto the unembedding vector of the source token S\n",
    "        4. Find the magnitude of the result\n",
    "\n",
    "    Why is this a good CS metric? Because it...\n",
    "\n",
    "        > Requires the QK circuit to exhibit prediction-attention (i.e. attend back to the tokens it is predicting)\n",
    "        > Requires the OV circuit to exhibit copy-suppression (i.e. suppress the prediction of the source token)\n",
    "        > Will be very small for any heads which don't strongly exhibit copy-suppression or copy-amplification (because we're \n",
    "          multiplying by attn and projecting onto a single direction)\n",
    "\n",
    "    Why might this not be a good CS metric? Because it...\n",
    "\n",
    "        > Will only pick up on \"pure copy-suppression\" rather than \"fuzzy copy-suppression\"\n",
    "            > But that's okay, because pure CS is still a good fraction of what the head does, and so we should still be able to identify the neg heads\n",
    "\n",
    "    What might be a better metric?\n",
    "\n",
    "        Filter for high attention probabilities (e.g. 10x more than 1/seqQ), then only keep that result vector if the corresonding unembedding is present\n",
    "        on the query-side. Not sure if this is better.\n",
    "    '''\n",
    "    toks = process_webtext_2(model, DATA_STR, SEQ_LEN)\n",
    "    batch_size, seq_len = toks.shape\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        toks,\n",
    "        return_type = None,\n",
    "        names_filter = lambda name: any(name.endswith(x) for x in [\"pattern\", \"v\", \"resid_post\", \"scale\"])\n",
    "    )\n",
    "\n",
    "    FUNCTION_TOKS = t.concat([\n",
    "        model.to_tokens(FUNCTION_STR_TOKS, prepend_bos=False).squeeze().to(device),\n",
    "        t.tensor([model.tokenizer.bos_token_id]).to(device)\n",
    "    ])\n",
    "\n",
    "    results = t.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    # Get all source token unembeddings\n",
    "    unembeddings_for_src_tokens: Float[Tensor, \"batch seqK\"] = model.W_U.T[toks]\n",
    "    # Get useful indices\n",
    "    batch_idx = einops.repeat(t.arange(batch_size), \"b -> b sQ\", sQ=seq_len)\n",
    "    seqQ_idx = einops.repeat(t.arange(seq_len), \"sQ -> b sQ sK\", b=batch_size, sK=seq_len)\n",
    "    seqK_idx = einops.repeat(t.arange(seq_len), \"sK -> b sQ sK\", b=batch_size, sQ=seq_len)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Get everything we need from the cache\n",
    "        resid_post_scaled: Float[Tensor, \"batch seqQ d_model\"] = cache[\"resid_post\", layer] / cache[\"scale\"]\n",
    "        v: Float[Tensor, \"batch seqK head d_head\"] = cache[\"v\", layer]\n",
    "        pattern: Float[Tensor, \"batch head seqQ seqK\"] = cache[\"pattern\", layer]\n",
    "\n",
    "        # Get logit lens for src tokens, at each dest token\n",
    "        logit_lens: Float[Tensor, \"batch seqQ seqK\"] = einops.einsum(\n",
    "            resid_post_scaled, unembeddings_for_src_tokens,\n",
    "            \"batch seqQ d_model, batch seqK d_model -> batch seqQ seqK\"\n",
    "        )\n",
    "        # Apply causal mask\n",
    "        logit_lens = t.where(seqQ_idx >= seqK_idx, logit_lens, -1e9)\n",
    "        # best_src_tok_seqpos[i, j] = the seqpos of the source token we're moving attn from, with destination token as (i, j)\n",
    "        best_src_tok_seqpos: Int[Tensor, \"batch seqQ\"] = logit_lens.argmax(-1)\n",
    "        best_src_toks: Int[Tensor, \"batch seqQ\"] = toks[batch_idx, best_src_tok_seqpos]\n",
    "\n",
    "        # Get the actual things we're moving\n",
    "\n",
    "        v_src: Float[Tensor, \"batch seqQ head d_head\"] = v[batch_idx, best_src_tok_seqpos]\n",
    "        pattern_src_dest: Float[Tensor, \"batch seqQ head\"] = pattern[batch_idx, :, seqQ_idx[..., 0], best_src_tok_seqpos]\n",
    "\n",
    "        # Do the projections & attention scaling\n",
    "        result_src: Float[Tensor, \"batch seqQ head d_model\"] = einops.einsum(\n",
    "            v_src, model.W_O[layer],\n",
    "            \"batch seqQ head d_head, head d_head d_model -> batch seqQ head d_model\"\n",
    "        )\n",
    "        result_src_projections: Float[Tensor, \"batch seqQ head\"] = einops.einsum(\n",
    "            result_src, unembeddings_for_src_tokens[batch_idx, best_src_tok_seqpos],\n",
    "            \"batch seqQ head d_model, batch seqQ d_model -> batch seqQ head\"\n",
    "        ) / cache[\"scale\"]\n",
    "        result_dest_projections: Float[Tensor, \"batch seqQ head\"] = result_src_projections * pattern_src_dest\n",
    "\n",
    "        # Get a filter for where the source token was a function word (we don't include these)\n",
    "        best_src_toks_are_fn_words: Bool[Tensor, \"batch seqQ 1\"] = (best_src_toks[:, :, None] == FUNCTION_TOKS[None, None, :]).any(dim=-1, keepdim=True)\n",
    "\n",
    "        # Get the results for all the non-fn words\n",
    "        mean_result_dest_projections = (result_dest_projections * ~best_src_toks_are_fn_words).sum(dim=(0, 1)) / (~best_src_toks_are_fn_words).sum(dim=(0, 1))\n",
    "        results[layer] = mean_result_dest_projections\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_vivo_copy_suppression_scores_2(model: HookedTransformer, DATA_STR: Int[Tensor, \"batch seq\"]):\n",
    "    '''\n",
    "    Same as the other one, except rather than picking the top source token, it picks the top token over all 50k words, and sets the result to zero\n",
    "    if that top token isn't in context. This is a lot more strict, and hopefully a lot more sparse.\n",
    "    '''\n",
    "    toks = process_webtext_2(model, DATA_STR, SEQ_LEN)\n",
    "    batch_size, seq_len = toks.shape\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        toks,\n",
    "        return_type = None,\n",
    "        names_filter = lambda name: any(name.endswith(x) for x in [\"pattern\", \"v\", \"resid_pre\", \"scale\"])\n",
    "    )\n",
    "\n",
    "    FUNCTION_TOKS = t.concat([\n",
    "        model.to_tokens(FUNCTION_STR_TOKS, prepend_bos=False).squeeze().to(device),\n",
    "        t.tensor([model.tokenizer.bos_token_id]).to(device)\n",
    "    ])\n",
    "\n",
    "    results = t.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Get everything we need from the cache\n",
    "        resid_post_scaled: Float[Tensor, \"batch seqQ d_model\"] = cache[\"resid_pre\", layer] / cache[\"scale\"]\n",
    "        v: Float[Tensor, \"batch seqK head d_head\"] = cache[\"v\", layer]\n",
    "        pattern: Float[Tensor, \"batch head seqQ seqK\"] = cache[\"pattern\", layer]\n",
    "\n",
    "        # Get logit lens for all tokens, at each dest token\n",
    "        # Get the (batch, seqQ) indices of everywhere that the top token isn't a function word & is in context\n",
    "        logit_lens: Float[Tensor, \"batch seqQ d_vocab\"] = resid_post_scaled @ model.W_U\n",
    "        top_predicted_token: Int[Tensor, \"batch seqQ\"] = logit_lens.argmax(-1)\n",
    "        top_predicted_token_is_non_fn_word: Bool[Tensor, \"batch seqQ\"] = (top_predicted_token[:, :, None] != FUNCTION_TOKS[None, :]).all(dim=-1)\n",
    "        top_predicted_token_rep = einops.repeat(top_predicted_token, \"batch seqQ -> batch seqQ seqK\", seqK=seq_len)\n",
    "        toks_rep = einops.repeat(toks, \"batch seqK -> batch seqQ seqK\", seqQ=seq_len)\n",
    "        toks_rep = t.where(t.tril(t.ones((seq_len, seq_len))).bool(), toks_rep, -1)\n",
    "        top_predicted_token_is_in_context: Bool[Tensor, \"batch seqQ\"] = (top_predicted_token_rep == toks_rep).any(dim=-1)\n",
    "        batch_seqQ_indices = t.nonzero(top_predicted_token_is_non_fn_word & top_predicted_token_is_in_context)\n",
    "\n",
    "        # If there are no destination tokens in the entire batch where the logit lens for this layer is a non-fn word in context, then skip\n",
    "        if batch_seqQ_indices.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # Now I have all the (batch_idx, dest_idx) s.t. I actually want to take the result from that token\n",
    "        batch_indices, seqQ_indices = batch_seqQ_indices.unbind(dim=-1)\n",
    "        seqK_indices = (top_predicted_token[:, :, None] == toks[:, None, :]).int().argmax(dim=-1)[batch_indices, seqQ_indices]\n",
    "        top_predicted_tokens = top_predicted_token[batch_indices, seqQ_indices]\n",
    "\n",
    "        # if layer == 10:\n",
    "        #     for b, sK, sQ in zip(batch_indices, seqK_indices, seqQ_indices):\n",
    "        #         # if \"Berk\" in model.to_single_str_token(toks[b, sQ].item()) + model.to_single_str_token(toks[b, sK].item()):\n",
    "        #         print(f\"[{b:02}] Dest = {model.to_single_str_token(toks[b, sQ].item())!r}, Src = {model.to_single_str_token(toks[b, sK].item())!r}\")\n",
    "\n",
    "        # Get the actual things we're moving\n",
    "        v_src: Float[Tensor, \"batch_seqQ head d_head\"] = v[batch_indices, seqK_indices]\n",
    "        pattern_src_dest: Float[Tensor, \"batch_seqQ head\"] = pattern[batch_indices, :, seqQ_indices, seqK_indices]\n",
    "\n",
    "        # Do the projections & attention scaling\n",
    "        result_src: Float[Tensor, \"batch_seqQ head d_model\"] = einops.einsum(\n",
    "            v_src, model.W_O[layer],\n",
    "            \"batch_seqQ head d_head, head d_head d_model -> batch_seqQ head d_model\"\n",
    "        )\n",
    "        result_src_projections: Float[Tensor, \"batch_seqQ head\"] = einops.einsum(\n",
    "            result_src, model.W_U.T[top_predicted_tokens],\n",
    "            \"batch_seqQ head d_model, batch_seqQ d_model -> batch_seqQ head\"\n",
    "        )\n",
    "        scale = cache[\"scale\"][batch_indices, seqQ_indices]\n",
    "        result_dest_projections: Float[Tensor, \"batch_seqQ head\"] = (result_src_projections * pattern_src_dest) / scale\n",
    "\n",
    "        # if layer == 10: print(batch_seqQ_indices)\n",
    "\n",
    "        # Scaling by the number of nonzero elements (because I expect)\n",
    "        results[layer] = einops.reduce(result_dest_projections, \"batch_seqQ head -> head\", \"mean\") * (toks.numel() / len(batch_indices))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_vivo_copy_suppression_scores_3(model: HookedTransformer, DATA_STR: Int[Tensor, \"batch seq\"]):\n",
    "    '''\n",
    "    Tried projecting the attention (query-side) onto the unembeddings. I think this intervention just doesn't work though, so I can ditch it.\n",
    "    '''\n",
    "\n",
    "    toks = process_webtext_2(model, DATA_STR, SEQ_LEN)\n",
    "    batch_size, seq_len = toks.shape\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        toks,\n",
    "        return_type = None,\n",
    "        names_filter = lambda name: any(name.endswith(x) for x in [\"v\", \"k\", \"resid_pre\", \"scale\"])\n",
    "    )\n",
    "\n",
    "    FUNCTION_TOKS = t.concat([\n",
    "        model.to_tokens(FUNCTION_STR_TOKS, prepend_bos=False).squeeze().to(device),\n",
    "        t.tensor([model.tokenizer.bos_token_id]).to(device)\n",
    "    ])\n",
    "\n",
    "    results = t.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    is_fn_word = (toks[:, :, None] == FUNCTION_TOKS[None, None, :]).any(dim=-1)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Get everything we need from the cache\n",
    "        resid_pre: Float[Tensor, \"batch seqQ d_model\"] = cache[\"resid_pre\", layer]\n",
    "        v: Float[Tensor, \"batch seqK head d_head\"] = cache[\"v\", layer]\n",
    "        k: Float[Tensor, \"batch seqK head d_head\"] = cache[\"k\", layer]\n",
    "\n",
    "        # Get projections so we can compute attention\n",
    "        resid_pre_projected_onto_unembeddings = project(resid_pre, model.W_U.T[toks].unsqueeze(-1))\n",
    "        # Compute attention, after doing this projection\n",
    "        q = einops.einsum(resid_pre_projected_onto_unembeddings, model.W_Q[layer], \"batch seqQ d_model, head d_model d_head -> batch seqQ head d_head\") + model.b_Q[layer]\n",
    "        attn_scores = einops.einsum(q, k, \"batch seqQ head d_head, batch seqK head d_head -> batch seqQ seqK\") / (model.cfg.d_head ** 0.5)\n",
    "        attn_scores_masked = t.where(t.tril(t.ones((seq_len, seq_len))).bool(), attn_scores, -1e9)\n",
    "        pattern = t.softmax(attn_scores_masked, dim=-1)\n",
    "\n",
    "        # Set v to be zero wherever the token is a function word\n",
    "        v_masked = t.where(is_fn_word[..., None, None], v, 0.0)\n",
    "        # Get results, and project them onto the unembeddings for that source token\n",
    "        result = einops.einsum(v_masked, model.W_O[layer], \"batch seqK head d_head, head d_head d_model -> batch seqK head d_model\")\n",
    "        result_projected = project(result, einops.repeat(model.W_U.T[toks], \"batch seqK d_head -> batch seqK head d_head 1\", head=model.cfg.n_heads), only_keep=\"neg\")\n",
    "\n",
    "        # Get the norm\n",
    "        result = einops.einsum(result_projected, pattern, \"batch seqK head d_model, batch seqQ seqK -> batch seqQ head d_model\")\n",
    "        result_avg_norm: Float[Tensor, \"head\"] = result.pow(2).sum(dim=-1).sqrt().mean(dim=(0, 1))\n",
    "\n",
    "        # Scaling by the number of nonzero elements (because I expect)\n",
    "        results[layer] = result_avg_norm\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got CS scores in 1.45s\n",
      "Got AI scores in 1.81s\n",
      "Got in-vivo scores in 1.17s\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly",
        "responsive": true
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.0010926540708169341,
           0
          ],
          [
           0,
           0,
           -0.0001383190246997401,
           0,
           0,
           -0.0002339325292268768,
           0,
           0,
           -0.0024863278958946466,
           -0.001018161652609706,
           -0.0002046096051344648,
           -0.000011216698112548329
          ],
          [
           0,
           0,
           0,
           0,
           0,
           -0.000003834008566627745,
           -0.000052349081670399755,
           -0.0007735719555057585,
           -1.712184172220077e-7,
           0,
           -0.0006469644722528756,
           -0.00009646896069170907
          ],
          [
           0,
           0,
           0,
           0,
           -0.000005682501068804413,
           0,
           0,
           0,
           -2.4619066607556306e-7,
           0,
           -0.00008080316183622926,
           0
          ],
          [
           0,
           0,
           0,
           0,
           -0.0004244917072355747,
           0,
           -0.00008672531839692965,
           -0.000029397513571893796,
           0,
           0,
           -0.00003179113264195621,
           0
          ],
          [
           0,
           0,
           -0.000022233814888750203,
           0,
           0,
           0,
           0,
           0,
           -0.00020999023399781436,
           -0.0005855074850842357,
           0,
           -0.0005783718079328537
          ],
          [
           0,
           0,
           0,
           -0.0002681783807929605,
           0,
           -0.0001776327844709158,
           -0.0015026101609691978,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           -0.0022714200895279646,
           0,
           0,
           -0.006422201171517372,
           0,
           0,
           -0.00223640538752079,
           0,
           0
          ],
          [
           -0.0003472341632004827,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.0028513860888779163,
           -0.04586825892329216,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           -0.019969668239355087,
           0,
           -0.0077057224698364735,
           0,
           0,
           -0.00001626371158636175,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -1,
           0,
           0,
           0,
           0
          ],
          [
           -0.012869724072515965,
           0,
           0,
           0,
           -0.019847322255373,
           -0.009629407897591591,
           0,
           -0.00035844024387188256,
           0,
           0,
           -0.6859193444252014,
           0
          ]
         ]
        },
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "1",
         "type": "heatmap",
         "xaxis": "x2",
         "yaxis": "y2",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.00034269093885086477,
           0,
           0
          ],
          [
           0,
           0,
           -0.00009382587450090796,
           -0.000012816757589462213,
           0,
           -0.0005102153518237174,
           0,
           0,
           -0.0020362103823572397,
           -0.0016408134251832962,
           -0.00010809078958118334,
           -0.00007599271339131519
          ],
          [
           0,
           0,
           0,
           0,
           0,
           -8.634206238866682e-8,
           -0.000058025194448418915,
           -0.0005711886915378273,
           0,
           0,
           0,
           -0.00002690173641894944
          ],
          [
           0,
           0,
           0,
           0,
           -0.0010992181487381458,
           0,
           0,
           0,
           -2.7379500888713437e-9,
           0,
           -0.00006296091305557638,
           0
          ],
          [
           0,
           0,
           0,
           0,
           -0.0012487638741731644,
           0,
           -0.000003886079412040999,
           -0.000015314682968892157,
           0,
           0,
           -0.00012884341413155198,
           -8.637199554852683e-23
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.004760534968227148,
           0,
           -0.0020739142782986164
          ],
          [
           0,
           0,
           -0.0015927805798128247,
           0,
           0,
           -0.0002375031472183764,
           -0.015837127342820168,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           -0.0029149658512324095,
           0,
           0,
           -0.01936229132115841,
           0,
           0,
           -0.00033907528268173337,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.006186215206980705,
           -0.12364999949932098,
           0
          ],
          [
           0,
           0,
           0,
           0,
           -0.008105126209557056,
           -0.01678144745528698,
           0,
           -0.006327234674245119,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -1,
           0,
           0,
           0,
           0
          ],
          [
           -0.006002760957926512,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.6297935843467712,
           0
          ]
         ]
        },
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "2",
         "type": "heatmap",
         "xaxis": "x3",
         "yaxis": "y3",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.0001408817188348621,
           0,
           -0.00009265910921385512
          ],
          [
           0,
           0,
           -0.0004258150584064424,
           -0.000057201850722776726,
           0,
           -0.001372732687741518,
           0,
           0,
           -0.0030033555813133717,
           -0.0010235076770186424,
           -0.0009281446691602468,
           -0.039222948253154755
          ],
          [
           0,
           0,
           0,
           0,
           0,
           -0.0017798806075006723,
           -0.0013600963866338134,
           -0.005791830364614725,
           0,
           0,
           -0.002092773327603936,
           -0.0023838593624532223
          ],
          [
           0,
           0,
           0,
           0,
           -0.001099522807635367,
           0,
           0,
           0,
           -0.001451384392566979,
           0,
           -0.03624485433101654,
           0
          ],
          [
           0,
           0,
           0,
           0,
           -0.03570949286222458,
           0,
           -0.0009165561059489846,
           -0.04459172859787941,
           0,
           0,
           -0.0015218014596030116,
           0
          ],
          [
           0,
           0,
           0,
           0,
           -0.003964240197092295,
           0,
           0,
           0,
           -0.004019218496978283,
           -0.003174257930368185,
           0,
           -0.10865320265293121
          ],
          [
           0,
           0,
           -0.0024629365652799606,
           0,
           0,
           -0.0522555448114872,
           -0.11723880469799042,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           -0.08227106928825378,
           0,
           0,
           -0.039568282663822174,
           0,
           0,
           -0.012933709658682346,
           0,
           0
          ],
          [
           -0.004853990860283375,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.05880139768123627,
           -0.33797597885131836,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           -0.20475678145885468,
           0,
           -0.020801885053515434,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -1,
           0,
           0,
           0,
           0
          ],
          [
           -0.03609945997595787,
           0,
           0,
           0,
           -0.025391478091478348,
           0,
           0,
           0,
           0,
           0,
           -0.5917566418647766,
           0
          ]
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {},
          "showarrow": false,
          "text": "Copy-suppression scores<br>(IOI)",
          "x": 0.15999999999999998,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {},
          "showarrow": false,
          "text": "Anti-induction scores<br>(rand)",
          "x": 0.49999999999999994,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {},
          "showarrow": false,
          "text": "Copy-suppression scores<br>(in-vivo)",
          "x": 0.8399999999999999,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "autosize": false,
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "modebar": {
         "add": [
          "drawline",
          "drawopenpath",
          "drawclosedpath",
          "drawcircle",
          "drawrect",
          "eraseshape"
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Scores"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          0.31999999999999995
         ],
         "scaleanchor": "y"
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.33999999999999997,
          0.6599999999999999
         ],
         "matches": "x"
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.6799999999999999,
          0.9999999999999999
         ],
         "matches": "x"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"2670b333-cec8-45f0-8a45-5288662d8386\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2670b333-cec8-45f0-8a45-5288662d8386\")) {                    Plotly.newPlot(                        \"2670b333-cec8-45f0-8a45-5288662d8386\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010926540708169341,0.0],[0.0,0.0,-0.0001383190246997401,0.0,0.0,-0.0002339325292268768,0.0,0.0,-0.0024863278958946466,-0.001018161652609706,-0.0002046096051344648,-1.1216698112548329e-05],[0.0,0.0,0.0,0.0,0.0,-3.834008566627745e-06,-5.2349081670399755e-05,-0.0007735719555057585,-1.712184172220077e-07,0.0,-0.0006469644722528756,-9.646896069170907e-05],[0.0,0.0,0.0,0.0,-5.682501068804413e-06,0.0,0.0,0.0,-2.4619066607556306e-07,0.0,-8.080316183622926e-05,0.0],[0.0,0.0,0.0,0.0,-0.0004244917072355747,0.0,-8.672531839692965e-05,-2.9397513571893796e-05,0.0,0.0,-3.179113264195621e-05,0.0],[0.0,0.0,-2.2233814888750203e-05,0.0,0.0,0.0,0.0,0.0,-0.00020999023399781436,-0.0005855074850842357,0.0,-0.0005783718079328537],[0.0,0.0,0.0,-0.0002681783807929605,0.0,-0.0001776327844709158,-0.0015026101609691978,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,-0.0022714200895279646,0.0,0.0,-0.006422201171517372,0.0,0.0,-0.00223640538752079,0.0,0.0],[-0.0003472341632004827,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0028513860888779163,-0.04586825892329216,0.0],[0.0,0.0,0.0,0.0,0.0,-0.019969668239355087,0.0,-0.0077057224698364735,0.0,0.0,-1.626371158636175e-05,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.0],[-0.012869724072515965,0.0,0.0,0.0,-0.019847322255373,-0.009629407897591591,0.0,-0.00035844024387188256,0.0,0.0,-0.6859193444252014,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"},{\"coloraxis\":\"coloraxis\",\"name\":\"1\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00034269093885086477,0.0,0.0],[0.0,0.0,-9.382587450090796e-05,-1.2816757589462213e-05,0.0,-0.0005102153518237174,0.0,0.0,-0.0020362103823572397,-0.0016408134251832962,-0.00010809078958118334,-7.599271339131519e-05],[0.0,0.0,0.0,0.0,0.0,-8.634206238866682e-08,-5.8025194448418915e-05,-0.0005711886915378273,0.0,0.0,0.0,-2.690173641894944e-05],[0.0,0.0,0.0,0.0,-0.0010992181487381458,0.0,0.0,0.0,-2.7379500888713437e-09,0.0,-6.296091305557638e-05,0.0],[0.0,0.0,0.0,0.0,-0.0012487638741731644,0.0,-3.886079412040999e-06,-1.5314682968892157e-05,0.0,0.0,-0.00012884341413155198,-8.637199554852683e-23],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.004760534968227148,0.0,-0.0020739142782986164],[0.0,0.0,-0.0015927805798128247,0.0,0.0,-0.0002375031472183764,-0.015837127342820168,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,-0.0029149658512324095,0.0,0.0,-0.01936229132115841,0.0,0.0,-0.00033907528268173337,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.006186215206980705,-0.12364999949932098,0.0],[0.0,0.0,0.0,0.0,-0.008105126209557056,-0.01678144745528698,0.0,-0.006327234674245119,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.0],[-0.006002760957926512,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.6297935843467712,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"},{\"coloraxis\":\"coloraxis\",\"name\":\"2\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0001408817188348621,0.0,-9.265910921385512e-05],[0.0,0.0,-0.0004258150584064424,-5.7201850722776726e-05,0.0,-0.001372732687741518,0.0,0.0,-0.0030033555813133717,-0.0010235076770186424,-0.0009281446691602468,-0.039222948253154755],[0.0,0.0,0.0,0.0,0.0,-0.0017798806075006723,-0.0013600963866338134,-0.005791830364614725,0.0,0.0,-0.002092773327603936,-0.0023838593624532223],[0.0,0.0,0.0,0.0,-0.001099522807635367,0.0,0.0,0.0,-0.001451384392566979,0.0,-0.03624485433101654,0.0],[0.0,0.0,0.0,0.0,-0.03570949286222458,0.0,-0.0009165561059489846,-0.04459172859787941,0.0,0.0,-0.0015218014596030116,0.0],[0.0,0.0,0.0,0.0,-0.003964240197092295,0.0,0.0,0.0,-0.004019218496978283,-0.003174257930368185,0.0,-0.10865320265293121],[0.0,0.0,-0.0024629365652799606,0.0,0.0,-0.0522555448114872,-0.11723880469799042,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,-0.08227106928825378,0.0,0.0,-0.039568282663822174,0.0,0.0,-0.012933709658682346,0.0,0.0],[-0.004853990860283375,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.05880139768123627,-0.33797597885131836,0.0],[0.0,0.0,0.0,0.0,0.0,-0.20475678145885468,0.0,-0.020801885053515434,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.0],[-0.03609945997595787,0.0,0.0,0.0,-0.025391478091478348,0.0,0.0,0.0,0.0,0.0,-0.5917566418647766,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x3\",\"yaxis\":\"y3\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.31999999999999995],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.33999999999999997,0.6599999999999999],\"matches\":\"x\"},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.6799999999999999,0.9999999999999999],\"matches\":\"x\"},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"annotations\":[{\"font\":{},\"showarrow\":false,\"text\":\"Copy-suppression scores\\u003cbr\\u003e(IOI)\",\"x\":0.15999999999999998,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"Anti-induction scores\\u003cbr\\u003e(rand)\",\"x\":0.49999999999999994,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"Copy-suppression scores\\u003cbr\\u003e(in-vivo)\",\"x\":0.8399999999999999,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Scores\"},\"modebar\":{\"add\":[\"drawline\",\"drawopenpath\",\"drawclosedpath\",\"drawcircle\",\"drawrect\",\"eraseshape\"]},\"autosize\":false},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2670b333-cec8-45f0-8a45-5288662d8386');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Getting CS scores...\", end=\"\\r\")\n",
    "t0 = time.time()\n",
    "cs_ioi_scores = get_copy_suppression_scores_ioi(gpt2, N=100)\n",
    "print(f\"Got CS scores in {time.time()-t0:.2f}s\")\n",
    "\n",
    "print(\"Getting AI scores...\", end=\"\\r\")\n",
    "t0 = time.time()\n",
    "ai_rand_scores = get_anti_induction_scores(gpt2, N=100)\n",
    "print(f\"Got AI scores in {time.time()-t0:.2f}s\")\n",
    "\n",
    "print(\"Getting in-vivo scores...\", end=\"\\r\")\n",
    "t0 = time.time()\n",
    "in_vivo_scores = get_in_vivo_copy_suppression_scores_2(gpt2, DATA_STR=DATA_STR)\n",
    "print(f\"Got in-vivo scores in {time.time()-t0:.2f}s\")\n",
    "\n",
    "all_results = t.stack([cs_ioi_scores, ai_rand_scores, in_vivo_scores])\n",
    "neg_results = all_results * (all_results < 0)\n",
    "neg_results_01 = neg_results / einops.reduce(-neg_results, \"stack layer head -> stack 1 1\", \"max\")\n",
    "\n",
    "imshow(\n",
    "    neg_results_01,\n",
    "    title = \"Scores\",\n",
    "    facet_col = 0,\n",
    "    facet_labels = [\"Copy-suppression scores<br>(IOI)\", \"Anti-induction scores<br>(rand)\", \"Copy-suppression scores<br>(in-vivo)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_scores(model_name: str, N: int, plot: bool = False):\n",
    "\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        model_name,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=\"cpu\"\n",
    "        # refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "    model.set_use_attn_result(False)\n",
    "\n",
    "    copy_suppression_scores_ioi = get_copy_suppression_scores_ioi(model, N)\n",
    "\n",
    "    anti_induction_scores = get_anti_induction_scores(model, N)\n",
    "\n",
    "    # in_vivo_copy_suppression_scores = get_in_vivo_copy_suppression_scores_2(model, DATA_STR)\n",
    "\n",
    "    model_scores = t.stack([copy_suppression_scores_ioi, anti_induction_scores]) # , in_vivo_copy_suppression_scores])\n",
    "\n",
    "    RESULTS_DIR = Path(\"/home/ubuntu/TransformerLens/transformer_lens/rs/callum2/explore_prompts/media/anti_induction\")\n",
    "\n",
    "    with open(RESULTS_DIR / f\"scores_{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_scores, f)\n",
    "\n",
    "    if plot:\n",
    "        neg_scores = model_scores * (model_scores < 0)\n",
    "        neg_scores_01 = neg_scores / einops.reduce(neg_scores.abs(), \"stack layer head -> stack 1 1\", \"max\")\n",
    "        imshow(\n",
    "            neg_scores_01, # neg_model_scores,\n",
    "            title=model_name,\n",
    "            facet_col=0,\n",
    "            text_auto=\".1f\",\n",
    "            width=600 + (400 * model_scores.shape[0]),\n",
    "            height=200 + (200 / 6) * model.cfg.n_layers,\n",
    "            static=True,\n",
    "            facet_labels=[\"IOI Copy Suppression DLA\", \"Anti-Induction DLA\"] # , \"CS-ablated norm\"], # , \"Funky weight scores\"]\n",
    "        )\n",
    "\n",
    "\n",
    "def aggregate_saved_scores(\n",
    "    overwrite: bool = False,\n",
    "    delete: bool = False,\n",
    "    show: bool = False,\n",
    "):\n",
    "    '''\n",
    "    Aggregates all saved scores in the anti_induction folder, into a single dictionary `scores.pkl`.\n",
    "\n",
    "    If `overwrite`, then it'll overwrite scores in the dictionary if they already exist.\n",
    "    If `delete`, then it'll delete all the individual score files (leaving only the dict).\n",
    "    '''\n",
    "    RESULTS_DIR = Path(\"/home/ubuntu/TransformerLens/transformer_lens/rs/callum2/explore_prompts/media/anti_induction\")\n",
    "    results_dict_orig = pickle.load(open(RESULTS_DIR / \"scores_dict.pkl\", \"rb\"))\n",
    "    results_dict_new = {\n",
    "        file.stem.replace(\"scores_\", \"\"): pickle.load(open(file, \"rb\"))\n",
    "        for file in RESULTS_DIR.glob(\"scores_*.pkl\")\n",
    "        if \"dict\" not in file.stem\n",
    "    }\n",
    "    if overwrite:\n",
    "        # The one on the right is the one which overrides the results\n",
    "        results_dict = {**results_dict_orig, **results_dict_new}\n",
    "    else:\n",
    "        results_dict = {**results_dict_new, **results_dict_orig}\n",
    "\n",
    "    if delete:\n",
    "        for file in RESULTS_DIR.glob(\"scores_*.pkl\"):\n",
    "            file.unlink()\n",
    "\n",
    "    if show:\n",
    "        print(\"OLD\")\n",
    "        for k in sorted(results_dict_orig.keys()): print(\"  \" + k)\n",
    "        print(\"\\nNEW\")\n",
    "        for k in sorted(results_dict_new.keys()): print(\"  \" + k)\n",
    "\n",
    "    pickle.dump(results_dict, open(RESULTS_DIR / \"scores_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_saved_scores(overwrite=True, delete=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_MODEL_NAMES = [\n",
    "    \"distillgpt2\",\n",
    "    \"gpt2-small\",\n",
    "    *[f\"stanford-gpt2-small-{i}\" for i in \"abcde\"],\n",
    "    *[f\"pythia-{n}m\" for n in [70, 160]],\n",
    "    *[f\"pythia-{n}m-deduped\" for n in [70, 160]],\n",
    "    *[f\"solu-{n}l\" for n in [4, 6, 8, 10]],\n",
    "    *[f\"solu-{n}l-pile\" for n in [4, 6, 8, 10]],\n",
    "    \"gelu-4l\",\n",
    "    \"gpt-neo-125m\",\n",
    "    \"opt-125m\",\n",
    "]\n",
    "MEDIUM_MODEL_NAMES = [\n",
    "    \"gpt-neo-125m\",\n",
    "    \"gpt2-medium\",\n",
    "    *[f\"stanford-gpt2-medium-{i}\" for i in \"abcde\"],\n",
    "    *[f\"pythia-{n}m\" for n in [410]],\n",
    "    *[f\"pythia-{n}m-deduped\" for n in [410]],\n",
    "    \"solu-12l\",\n",
    "    \"gpt2-large\",\n",
    "]\n",
    "BIG_MODEL_NAMES = [\n",
    "    *[f\"pythia-{n}b\" for n in [1.4, 2.8]],\n",
    "    *[f\"pythia-{n}b-deduped\" for n in [1.4, 2.8]],\n",
    "    \"gpt2-xl\",\n",
    "    \"gpt-neo-2.7B\",\n",
    "    \"opt-1.3b\",\n",
    "    \"opt-2.7b\",\n",
    "]\n",
    "GIANT_MODEL_NAMES = [\n",
    "    *[f\"pythia-{n}b\" for n in [6.9]],\n",
    "    *[f\"pythia-{n}b-deduped\" for n in [6.9]],\n",
    "    \"gpt-j-6B\",\n",
    "    \"opt-6.7b\",\n",
    "]\n",
    "BROBDINGNAGIAN_MODEL_NAMES = [\n",
    "    *[f\"pythia-{n}b\" for n in [12]],\n",
    "    *[f\"pythia-{n}b-deduped\" for n in [12]],\n",
    "    \"gpt-neox-20b\",\n",
    "    \"opt-13b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 mins (not inc. initial loading, metrics = CS/ioi + AI/rand + CS/norm)\n",
    "for model_name in SMALL_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=True)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 minutes (including initial model loading, metrics = CS/ioi + AI/rand)\n",
    "# 5.9-3.9 minutes (not inc.  initial model loading, metrics = CS/ioi + AI/rand + CS/norm)\n",
    "for model_name in MEDIUM_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=True)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.9 minutes (including initial model loading, metrics = CS/ioi + AI/rand)\n",
    "# 11.9-9.5 minutes (not including initial model loading, metrics = CS/ioi + AI/rand + CS/norm)\n",
    "for model_name in BIG_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20.6 minutes (including initial model loading)\n",
    "for model_name in GIANT_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in BROBDINGNAGIAN_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = \"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/anti_induction_vs_copy_suppression/model_results\"\n",
    "# old = \"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/streamlit/anti_induction_vs_copy_suppression/model_results\"\n",
    "\n",
    "# new = list(map(lambda x: x.name, Path(new).iterdir()))\n",
    "# old = list(map(lambda x: x.name, Path(old).iterdir()))\n",
    "\n",
    "# set(new) - set(old)\n",
    "# set(old) - set(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_all_results():\n",
    "    results_copy_suppression_ioi = []\n",
    "    results_anti_induction = []\n",
    "    model_names = []\n",
    "    head_names = []\n",
    "\n",
    "    RESULTS_DIR = Path(\"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/anti_induction_vs_copy_suppression/model_results\")\n",
    "\n",
    "    for file in RESULTS_DIR.iterdir():\n",
    "        with open(file, \"rb\") as f:\n",
    "            model_scores: Tensor = pickle.load(f)\n",
    "\n",
    "            for layer in range(model_scores.size(1)):\n",
    "                for head in range(model_scores.size(2)):\n",
    "                    results_copy_suppression_ioi.append(model_scores[0, layer, head].item())\n",
    "                    results_anti_induction.append(model_scores[1, layer, head].item())\n",
    "                    model_names.append(file.stem.replace(\"scores_\", \"\"))\n",
    "                    head_names.append(f\"{layer}.{head}\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"results_copy_suppression_ioi\": results_copy_suppression_ioi,\n",
    "        \"results_anti_induction\": results_anti_induction,\n",
    "        \"model_names\": model_names,\n",
    "        \"head_names\": head_names\n",
    "    })\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"results_copy_suppression_ioi\", y=\"results_anti_induction\", color='model_names', hover_data=[\"model_names\", \"head_names\"],\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        title=\"Anti-Induction Scores (repeated random tokens) vs Copy Suppression Scores (IOI)\",\n",
    "        labels={\"results_copy_suppression_ioi\": \"Copy Suppression\", \"results_anti_induction\": \"Anti-Induction\"}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_all_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
