{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Note - I couldn't figure out how to fix local imports for a while. Solution ended up being to make sure that this version of `transformer_lens` is before my libraries in `sys.path` (hence I'm inserting it at position zero in the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"C:\\Users\\calsm\\Documents\\AI Alignment\\SERIMATS_23\\seri_mats_23_streamlit_pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if (sys.path[0] != str_p):\n",
    "        sys.path.insert(0, str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "import gzip\n",
    "\n",
    "from transformer_lens.rs.callum2.cspa.cspa_functions import (\n",
    "    FUNCTION_STR_TOKS,\n",
    "    # project,\n",
    "    # get_effective_embedding,\n",
    "    get_cspa_results,\n",
    "    get_cspa_results_batched,\n",
    "    # first_occurrence_2d,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_utils import (\n",
    "    parse_str,\n",
    "    parse_str_toks_for_printing,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_plots import (\n",
    "    generate_scatter,\n",
    "    generate_loss_based_scatter,\n",
    "    add_cspa_to_streamlit_page,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.utils import (\n",
    "    ST_HTML_PATH,\n",
    "    parse_str_tok_for_printing,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.model_results import (\n",
    "    get_model_results,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.generate_html_funcs import (\n",
    "    # generate_html_for_cspa_plots,\n",
    "    # generate_html_for_logit_plot,\n",
    "    # generate_html_for_DLA_plot,\n",
    "    generate_4_html_plots,\n",
    "    CSS,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_semantic_similarity import (\n",
    "    get_equivalency_toks,\n",
    "    get_related_words,\n",
    "    concat_lists,\n",
    "    # is_token,\n",
    "    # get_list_with_no_repetitions,\n",
    "    make_list_correct_length,\n",
    "    create_full_semantic_similarity_dict,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\" # \"cuda\"\n",
    "    # fold value bias?\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000 # 80 for viz?\n",
    "SEQ_LEN = 61 # 70 for viz (no more, because attn)\n",
    "batch_idx = 36\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "\n",
    "def process_webtext(\n",
    "    seed: int = 6,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    indices: Optional[List[int]] = None,\n",
    "    seq_len: int = SEQ_LEN,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    DATA_STR = get_webtext(seed=seed)\n",
    "    if indices is None:\n",
    "        DATA_STR = DATA_STR[:batch_size]\n",
    "    else:\n",
    "        DATA_STR = [DATA_STR[i] for i in indices]\n",
    "    DATA_STR = [parse_str(s) for s in DATA_STR]\n",
    "\n",
    "    DATA_TOKS = model.to_tokens(DATA_STR)\n",
    "    DATA_STR_TOKS = model.to_str_tokens(DATA_STR)\n",
    "\n",
    "    if seq_len < 1024:\n",
    "        DATA_TOKS = DATA_TOKS[:, :seq_len]\n",
    "        DATA_STR_TOKS = [str_toks[:seq_len] for str_toks in DATA_STR_TOKS]\n",
    "\n",
    "    DATA_STR_TOKS_PARSED = list(map(parse_str_toks_for_printing, DATA_STR_TOKS))\n",
    "\n",
    "    clear_output()\n",
    "    if verbose:\n",
    "        print(f\"Shape = {DATA_TOKS.shape}\\n\")\n",
    "        print(\"First prompt:\\n\" + \"\".join(DATA_STR_TOKS[0]))\n",
    "\n",
    "    return DATA_TOKS, DATA_STR_TOKS_PARSED\n",
    "\n",
    "\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED = process_webtext(verbose=True)\n",
    "DATA_TOKS_MINI = DATA_TOKS[[32, 36], :60]\n",
    "DATA_STR_TOKS_PARSED_MINI = [DATA_STR_TOKS_PARSED[32][:60], DATA_STR_TOKS_PARSED[36][:60]]\n",
    "\n",
    "# DATA_TOKS_SMALL, DATA_STR_TOKS_PARSED_SMALL = process_webtext(indices=list(range(32, 42)), verbose=True) # \n",
    "# BATCH_SIZE, SEQ_LEN = DATA_TOKS.shape\n",
    "\n",
    "# MINIBATCH_SIZE = BATCH_SIZE // 3\n",
    "# DATA_TOKS_1 = DATA_TOKS[:MINIBATCH_SIZE]\n",
    "# DATA_TOKS_2 = DATA_TOKS[MINIBATCH_SIZE:2*MINIBATCH_SIZE]\n",
    "# DATA_TOKS_3 = DATA_TOKS[2*MINIBATCH_SIZE:]\n",
    "# DATA_STR_TOKS_PARSED_1 = DATA_STR_TOKS_PARSED[:MINIBATCH_SIZE]\n",
    "# DATA_STR_TOKS_PARSED_2 = DATA_STR_TOKS_PARSED[MINIBATCH_SIZE:2*MINIBATCH_SIZE]\n",
    "# DATA_STR_TOKS_PARSED_3 = DATA_STR_TOKS_PARSED[2*MINIBATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take tokens, and return the tokens of semantically similar words. \n",
    "\n",
    "There are 3 categories of semantically similar tokens `s*` for any given token `s`:\n",
    "\n",
    "1. Equivalence relations - this captures things like plurals, tokenization, capitalization.\n",
    "2. Superstrings - for instance, of you have `\"keley\"` this gives you `\" Berkeley\"`.\n",
    "3. Substrings - for instance, of you have `\" Berkeley\"` this gives you `\"keley\"`.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "1. For each token, generate all (1), and then see which ones actually split into multiple tokens (these become (3)).\n",
    "2. Iterate through this entire dict to generate (2)s for every token (this is basically like flipping the arrows in the other direction).\n",
    "\n",
    "### Problems with this method\n",
    "\n",
    "There are 4 problems with this method. I think (1) and (3) are the most problematic.\n",
    "\n",
    "1. The pluralization isn't sufficiently flexible, and it'll miss out on categories of things, for example:\n",
    "    * `\" write\"` and `\" writing\"` and `\" writer\"`\n",
    "    * `\" rental\"` and `\" rented\"` and `\" renting\"`\n",
    "    * (OV and QK circuits show that these do suppress each other)\n",
    "    * Possible solution - more hardcoded rules?\n",
    "2. Misses some important things which aren't semantically similar as we've defined it, e.g. `1984` and `1985` aren't semantically similar (OV and QK circuits show that they do suppress each other)\n",
    "    * Possible solution - ???\n",
    "\n",
    "However, this maybe isn't worth further optimization, because it doesn't marginally improve the results by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.text.en import conjugate, PRESENT, PAST, FUTURE, SUBJUNCTIVE, INFINITIVE, PROGRESSIVE, PLURAL, SINGULAR\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "MY_TENSES = [PRESENT, PAST, FUTURE, SUBJUNCTIVE, INFINITIVE, PROGRESSIVE]\n",
    "MY_NUMBERS = [PLURAL, SINGULAR]\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cspa_semantic_dict_reversed, cspa_semantic_dict = create_full_semantic_similarity_dict(model, full_version=True)\n",
    "\n",
    "# with open(ST_HTML_PATH.parent.parent / \"cspa/cspa_semantic_dict_full.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cspa_semantic_dict, f)\n",
    "\n",
    "cspa_semantic_dict = pickle.load(open(ST_HTML_PATH.parent.parent / \"cspa/cspa_semantic_dict_full.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h2>Related words</h2>\"))\n",
    "\n",
    "for word in [\"Berkeley\", \"pier\", \"pie\", \"ring\", \"device\", \"robot\", \"w\"]:\n",
    "    print(get_related_words(word, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h2>Equivalency tokens</h2>\"))\n",
    "\n",
    "for tok in [\" Berkeley\", \" Pier\", \" pier\", \"pie\", \" pies\", \" ring\", \" device\", \" robot\", \"w\"]:\n",
    "    print(f\"{tok!r:>10} -> {get_equivalency_toks(tok, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table(\"Source token\", \"Top 3 related\", \"All semantically related\", title=\"Semantic similarity: bidirectional, superstrings, substrings\")\n",
    "\n",
    "str_toks = [\" Berkeley\", \"keley\", \" University\", \" Mary\", \" Pier\", \" pier\", \"NY\", \" ring\", \" W\", \" device\", \" robot\", \" jump\", \" driver\", \" Cairo\"]\n",
    "print_cutoff = 70\n",
    "def cutoff(s):\n",
    "    if len(s_str := str(s)) >= print_cutoff: return s_str[:print_cutoff-4] + ' ...'\n",
    "    else: return s_str\n",
    "\n",
    "for str_tok in str_toks:\n",
    "    top3_sim = \"\\n\".join(list(map(repr, concat_lists(cspa_semantic_dict[str_tok])[:3])))\n",
    "    bidir, superstr, substr = cspa_semantic_dict[str_tok]\n",
    "    all_sim = \"\\n\".join([\n",
    "        cutoff(f\"{len(bidir)} Bidir: {bidir}\"),\n",
    "        cutoff(f\"{len(superstr)} Super: {superstr}\"),\n",
    "        cutoff(f\"{len(substr)} Sub:   {substr}\"),\n",
    "    ]) + \"\\n\"\n",
    "    table.add_row(repr(str_tok), top3_sim, all_sim)\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running CSPA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't preserve the BOS stuff\n",
    "\n",
    "cspa_results_1, s_sstar_pairs_1 = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS, # [:50],\n",
    "    max_batch_size = 100, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    components_to_project = [\"o\"],\n",
    "    K_unembeddings = 8, # None\n",
    "    K_semantic = 1, # 3\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    effective_embedding = \"W_E (including MLPs)\",\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    return_dla = True,\n",
    "    start_idx = 0,\n",
    ")\n",
    "# TODO - speed of projection onto large number of vectors is the main bottleneck by far; try to speed it up\n",
    "\n",
    "fig_dict = generate_scatter(cspa_results_1, DATA_STR_TOKS_PARSED, batch_index_colors_to_highlight=[51, 300])\n",
    "fig_loss_line = generate_loss_based_scatter(cspa_results_1, nbins=200)\n",
    "\n",
    "# ! Save figures for ST page\n",
    "# fig_dict[\"loss-based scatter\"] = fig_loss_line\n",
    "# p = \"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media\"\n",
    "# with gzip.open(f\"{p}/CSPA_figs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fig_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't preserve the BOS stuff\n",
    "\n",
    "cspa_results, s_sstar_pairs = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS, # [:50],\n",
    "    max_batch_size = 100, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    components_to_project = [\"o\"],\n",
    "    K_unembeddings = 8, # None\n",
    "    K_semantic = 5, # 3\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    effective_embedding = \"W_E (including MLPs)\",\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    return_dla = True,\n",
    "    start_idx = 0,\n",
    ")\n",
    "# TODO - speed of projection onto large number of vectors is the main bottleneck by far; try to speed it up\n",
    "\n",
    "fig_dict = generate_scatter(cspa_results, DATA_STR_TOKS_PARSED, batch_index_colors_to_highlight=[51, 300])\n",
    "fig_loss_line = generate_loss_based_scatter(cspa_results, nbins=200)\n",
    "\n",
    "# ! Save figures for ST page\n",
    "fig_dict[\"loss-based scatter\"] = fig_loss_line\n",
    "p = \"/root/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media\"\n",
    "with gzip.open(f\"{p}/CSPA_figs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fig_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding CSPA to the Streamlit page (\"Browse Examples\")\n",
    "\n",
    "This code adds the CSPA plots to the HTML plots for the Streamlit page. It creates a 5th tab called `CSPA`, and adds to the logit and DLA plots in the second tab (the latter is mainly for our use, while we're iterating on and improving the CSPA code).\n",
    "\n",
    "I've added to this code in a pretty janky way, so that it can show more than one CSPA plot stacked on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cspa_to_streamlit_page(\n",
    "    cspa_results = {\"$K_{sem}=4$\": cspa_results, \"$K_{sem}=1$\": cspa_results_1},\n",
    "    s_sstar_pairs = {\"$K_{sem}=4$\": s_sstar_pairs, \"$K_{sem}=1$\": s_sstar_pairs_1},\n",
    "    html_plots_filename = f\"GZIP_HTML_PLOTS_b301_s61.pkl\",\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    toks_for_doing_DLA = DATA_TOKS,\n",
    "    model = model,\n",
    "    verbose = True,\n",
    "    test_idx = 40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 51\n",
    "add_cspa_to_streamlit_page(\n",
    "    cspa_results = cspa_results,\n",
    "    s_sstar_pairs = s_sstar_pairs,\n",
    "    html_plots_filename = f\"GZIP_HTML_PLOTS_b{b}_s61.pkl\",\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    toks_for_doing_DLA = DATA_TOKS,\n",
    "    model = model,\n",
    "    verbose = True,\n",
    "    # test_idx = 32,\n",
    ")\n",
    "\n",
    "b = 200\n",
    "add_cspa_to_streamlit_page(\n",
    "    cspa_results = {\"k=4\": cspa_results, \"k=1\": cspa_results_1},\n",
    "    s_sstar_pairs = {\"k=4\": s_sstar_pairs, \"k=1\": s_sstar_pairs_1},\n",
    "    html_plots_filename = f\"GZIP_HTML_PLOTS_b{b}_s61.pkl\",\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    toks_for_doing_DLA = DATA_TOKS,\n",
    "    model = model,\n",
    "    verbose = True,\n",
    "    # test_idx = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_of_toks(\n",
    "    toks1: List[str],\n",
    "    toks2: List[str],\n",
    "):\n",
    "    U1 = model.W_U.T[model.to_tokens(toks1, prepend_bos=False).squeeze()]\n",
    "    U2 = model.W_U.T[model.to_tokens(toks2, prepend_bos=False).squeeze()]\n",
    "\n",
    "    if U1.ndim == 1: U1 = U1.unsqueeze(0)\n",
    "    if U2.ndim == 1: U2 = U2.unsqueeze(0)\n",
    "\n",
    "    U1_normed = U1 / t.norm(U1, dim=-1, keepdim=True)\n",
    "    U2_normed = U2 / t.norm(U2, dim=-1, keepdim=True)\n",
    "\n",
    "    imshow(\n",
    "        U1_normed @ U2_normed.T,\n",
    "        title = \"Cosine similarity of unembeddings\",\n",
    "        x = toks2,\n",
    "        y = toks1,\n",
    "    )\n",
    "\n",
    "cos_sim_of_toks(\n",
    "    [\" stuff\"],\n",
    "    [\" devices\", \" phones\", \" screens\", \" device\", \" phone\", \" Android\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick code to get mean ablations for the streamlit page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TOKS, DATA_STR_TOKS_PARSED = process_webtext(verbose=False, batch_size=1000, seq_len=100)\n",
    "assert DATA_TOKS.shape == (1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all = t.empty((2, 0, 100, model.cfg.d_model))\n",
    "\n",
    "for i in range(0, 1000, 100):\n",
    "    _, cache = model.run_with_cache(\n",
    "        DATA_TOKS[i: i+100],\n",
    "        return_type=None,\n",
    "        names_filter=lambda name: name in [utils.get_act_name(\"result\", layer) for layer in [10, 11]]\n",
    "    )\n",
    "    result_L10H7 = cache[\"result\", 10][:, :, 7].cpu() # [batch seq d_model]\n",
    "    result_L11H10 = cache[\"result\", 11][:, :, 10].cpu() # [batch seq d_model]\n",
    "    result_all = t.cat([\n",
    "        result_all, \n",
    "        t.stack([result_L10H7, result_L11H10])\n",
    "    ], dim=1)\n",
    "\n",
    "result_mean = result_all.mean(dim=1)\n",
    "assert result_mean.shape == (2, 100, model.cfg.d_model)\n",
    "\n",
    "t.save(result_mean, f\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media/result_mean.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the code for \"love and war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mean_as_tensor = t.load(ST_HTML_PATH / \"result_mean.pt\")\n",
    "result_mean = {(10, 7): result_mean_as_tensor[0], (11, 10): result_mean_as_tensor[1]}\n",
    "\n",
    "prompt = \"I picked up the first box. I picked up the second box. I picked up the third and final box.\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "# Parse the string tokens for printing\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "model_results = get_model_results(\n",
    "    model,\n",
    "    toks=toks,\n",
    "    negative_heads=[(10, 7), (11, 10)],\n",
    "    result_mean=result_mean,\n",
    "    verbose=False\n",
    ")\n",
    "HTML_PLOTS_NEW = generate_4_html_plots(\n",
    "    model=model,\n",
    "    data_toks=toks,\n",
    "    data_str_toks_parsed=str_toks_parsed,\n",
    "    negative_heads=[(10, 7), (11, 10)],\n",
    "    model_results=model_results,\n",
    "    save_files=False,\n",
    ")\n",
    "cspa_results, s_sstar_pairs = get_cspa_results(\n",
    "    model=model,\n",
    "    toks=toks,\n",
    "    negative_head=(10, 7), #  this currently doesn't do anything; it's always 10.7\n",
    "    components_to_project=[\"o\"],\n",
    "    K_unembeddings=5,\n",
    "    K_semantic=3,\n",
    "    semantic_dict=cspa_semantic_dict,\n",
    "    effective_embedding=\"W_E (including MLPs)\",\n",
    "    result_mean=result_mean,\n",
    "    use_cuda=False,\n",
    "    return_dla=True,\n",
    ")\n",
    "HTML_PLOTS_NEW = add_cspa_to_streamlit_page(\n",
    "    cspa_results=cspa_results,\n",
    "    s_sstar_pairs=s_sstar_pairs,\n",
    "    data_str_toks_parsed=str_toks_parsed,\n",
    "    model=model,\n",
    "    HTML_PLOTS=HTML_PLOTS_NEW,\n",
    "    toks_for_doing_DLA=toks,\n",
    "    verbose=False,\n",
    "    test_idx=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
