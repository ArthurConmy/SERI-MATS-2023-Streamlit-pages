{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Prompts\n",
    "\n",
    "This is the notebook I use to test out the functions in this directory, and generate the plots in the Streamlit page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as t\n",
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"gpt2-small\",\n",
    "#     center_unembed=True,\n",
    "#     center_writing_weights=True,\n",
    "#     fold_ln=True,\n",
    "#     device=\"cpu\"\n",
    "#     # refactor_factored_attn_matrices=True,\n",
    "# )\n",
    "# model.set_use_split_qkv_input(False)\n",
    "# model.set_use_attn_result(True)\n",
    "\n",
    "# t.save(model.half(), \"gpt2-small.pt\")\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "import gzip\n",
    "\n",
    "from transformer_lens.rs.callum2.ov_qk_circuits.ov_qk_plot_functions import (\n",
    "    plot_logit_lens,\n",
    "    plot_full_matrix_histogram,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.generate_html_funcs import (\n",
    "    CSS,\n",
    "    generate_4_html_plots,\n",
    "    generate_4_html_plots_batched,\n",
    "    generate_html_for_DLA_plot,\n",
    "    generate_html_for_logit_plot,\n",
    "    generate_html_for_loss_plot,\n",
    "    generate_html_for_unembedding_components_plot,\n",
    "    attn_filter,\n",
    "    _get_color,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.model_results import (\n",
    "    get_model_results,\n",
    "    HeadResults,\n",
    "    LayerResults,\n",
    "    DictOfHeadResults,\n",
    "    ModelResults,\n",
    "    first_occurrence,\n",
    "    project,\n",
    "    model_fwd_pass_from_resid_pre,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.utils import (\n",
    "    create_title_and_subtitles,\n",
    "    parse_str,\n",
    "    parse_str_tok_for_printing,\n",
    "    parse_str_toks_for_printing,\n",
    "    topk_of_Nd_tensor,\n",
    "    ST_HTML_PATH,\n",
    ")\n",
    "from transformer_lens.rs.callum2.ioi_and_bos.ioi_functions import (\n",
    "    get_effective_embedding_2,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cpu\" # \"cuda\"\n",
    "    # fold value bias?\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE_dict = get_effective_embedding_2(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300 # 51 for viz\n",
    "SEQ_LEN = 61 # (61 for viz, no more because attn)\n",
    "batch_idx = 36\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "\n",
    "def process_webtext(\n",
    "    seed: int = 6,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    indices: Optional[List[int]] = None,\n",
    "    seq_len: int = SEQ_LEN,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    DATA_STR = get_webtext(seed=seed)\n",
    "    if indices is None:\n",
    "        DATA_STR = DATA_STR[:batch_size]\n",
    "    else:\n",
    "        DATA_STR = [DATA_STR[i] for i in indices]\n",
    "    DATA_STR = [parse_str(s) for s in DATA_STR]\n",
    "\n",
    "    DATA_TOKS = model.to_tokens(DATA_STR)\n",
    "    DATA_STR_TOKS = model.to_str_tokens(DATA_STR)\n",
    "\n",
    "    if seq_len < 1024:\n",
    "        DATA_TOKS = DATA_TOKS[:, :seq_len]\n",
    "        DATA_STR_TOKS = [str_toks[:seq_len] for str_toks in DATA_STR_TOKS]\n",
    "\n",
    "    DATA_STR_TOKS_PARSED = list(map(parse_str_toks_for_printing, DATA_STR_TOKS))\n",
    "\n",
    "    clear_output()\n",
    "    if verbose:\n",
    "        print(f\"Shape = {DATA_TOKS.shape}\\n\")\n",
    "        print(\"First prompt:\\n\" + \"\".join(DATA_STR_TOKS[0]))\n",
    "\n",
    "    return DATA_TOKS, DATA_STR_TOKS_PARSED\n",
    "\n",
    "\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED = process_webtext(verbose=True) # indices=list(range(32, 40))\n",
    "BATCH_SIZE, SEQ_LEN = DATA_TOKS.shape\n",
    "\n",
    "MINIBATCH_SIZE = BATCH_SIZE // 3\n",
    "DATA_TOKS_1 = DATA_TOKS[:MINIBATCH_SIZE]\n",
    "DATA_TOKS_2 = DATA_TOKS[MINIBATCH_SIZE:2*MINIBATCH_SIZE]\n",
    "DATA_TOKS_3 = DATA_TOKS[2*MINIBATCH_SIZE:]\n",
    "DATA_STR_TOKS_PARSED_1 = DATA_STR_TOKS_PARSED[:MINIBATCH_SIZE]\n",
    "DATA_STR_TOKS_PARSED_2 = DATA_STR_TOKS_PARSED[MINIBATCH_SIZE:2*MINIBATCH_SIZE]\n",
    "DATA_STR_TOKS_PARSED_3 = DATA_STR_TOKS_PARSED[2*MINIBATCH_SIZE:]\n",
    "\n",
    "# DATA_TOKS_MINI = DATA_TOKS[[32, 36], :60]\n",
    "# DATA_STR_TOKS_PARSED_MINI = [DATA_STR_TOKS_PARSED[32][:60], DATA_STR_TOKS_PARSED[36][:60]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test html in small case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "prompt = \"All's fair in love and war\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "MODEL_RESULTS = get_model_results(model, toks, NEGATIVE_HEADS)\n",
    "\n",
    "HTML_PLOTS = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = toks,\n",
    "    data_str_toks_parsed = str_toks_parsed,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = False,\n",
    ")\n",
    "\n",
    "for k, v in HTML_PLOTS.items():\n",
    "    print(k)\n",
    "    k2 = list(zip(*HTML_PLOTS[\"LOSS\"].keys()))\n",
    "    for j, _k2 in enumerate(k2):\n",
    "        print(f\"{(j)} = {sorted(set(_k2))}\")\n",
    "\n",
    "display(HTML(CSS + HTML_PLOTS[\"LOSS\"][(0, \"10.7\", \"direct+unfrozen+mean\", True)] + \"<br>\" * 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_4_html_plots_batched(\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS, # [:51],\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    max_batch_size = 100,\n",
    "    start_idx = 0,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_4_html_plots_batched(\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS[:51],\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    max_batch_size = 100,\n",
    "    start_idx = 0,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "neg = False\n",
    "all_ranks = []\n",
    "\n",
    "model.reset_hooks()\n",
    "logits, cache = model.run_with_cache(DATA_TOKS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_to_plot = [\n",
    "#     (35, 39, \" About\"),\n",
    "#     (67, 21, \" delays\"),\n",
    "#     (8, 35, \" rentals\"),\n",
    "#     (8, 54, \" require\"),\n",
    "#     (53, 18, [\" San\", \" Francisco\"]),\n",
    "#     (33, 9, \" Hollywood\"),\n",
    "#     (49, 7, \" Home\"),\n",
    "#     (71, 34, \" sound\"),\n",
    "#     (14, 56, \" Kara\"),\n",
    "# ]\n",
    "# points_to_plot = [\n",
    "#     (45, 42, [\" editorial\"]),\n",
    "#     (45, 58, [\" stadium\", \" Stadium\", \" stadiums\"]),\n",
    "#     (43, 56, [\" Biden\"]),\n",
    "#     (43, 44, [\" interview\", \" campaign\"]),\n",
    "#     (38, 54, [\" Mary\", \" Catholics\"]),\n",
    "#     (33, 29, \" Hollywood\"),\n",
    "#     (33, 42, \" BlackBerry\"),\n",
    "#     (31, 33, [\" Church\", \" churches\"]),\n",
    "#     (28, 53, [\" mobile\", \" phone\", \" device\"]),\n",
    "#     (25, 32, [\" abstraction\", \" abstract\", \" Abstract\"]),\n",
    "#     (18, 25, [\"TPP\", \" Lee\"]),\n",
    "#     (10, 52, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 52, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 35, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 25, [\" Italian\", \" Italy\"]),\n",
    "#     (6, 52, [\" landfill\", \" waste\"]),\n",
    "#     (4, 52, \" jury\"),\n",
    "# ]\n",
    "points_to_plot = [\n",
    "    # (14, 56, \" Kara\"),\n",
    "    # (67, 47, \" case\"),\n",
    "    # (24, 73, \" negotiation\"),\n",
    "    (2, 35, [\" Berkeley\", \"keley\"]),\n",
    "]\n",
    "\n",
    "resid_pre_head = (cache[\"resid_pre\", 10]) / cache[\"scale\", 10, \"ln1\"]  #  - cache[\"resid_pre\", 1]\n",
    "\n",
    "plot_logit_lens(points_to_plot, resid_pre_head, model, DATA_STR_TOKS_PARSED_2, k=15, title=\"Predictions at token ' of', before head 10.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: QK and OV circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dest, src, focus_on: Literal[\"src\", \"dest\"]):\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"OV\", neg=True, head=(10, 7), flip=(focus_on==\"dest\"))\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"QK\", neg=False, head=(10, 7), flip=(focus_on==\"src\"))\n",
    "\n",
    "plot_both(dest=\" Berkeley\", src=\"keley\", focus_on=\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dest, src, focus_on: Literal[\"src\", \"dest\"]):\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"OV\", neg=True, head=(10, 7), flip=(focus_on==\"dest\"))\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"QK\", neg=False, head=(10, 7), flip=(focus_on==\"src\"))\n",
    "\n",
    "plot_both(dest=\" negotiation\", src=\" negotiations\", focus_on=\"dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_orig = MODEL_RESULTS.logits_orig[32, 19].log_softmax(-1)\n",
    "logprobs_abl = MODEL_RESULTS.logits[(\"direct\", \"frozen\", \"mean\")][10, 7][32, 19].log_softmax(-1)\n",
    "\n",
    "logprobs_orig_topk = logprobs_orig.topk(10, dim=-1, largest=True)\n",
    "y_orig = logprobs_orig_topk.values.tolist()\n",
    "x = logprobs_orig_topk.indices\n",
    "y_abl = logprobs_abl[x].tolist()\n",
    "x = list(map(repr, model.to_str_tokens(x)))\n",
    "\n",
    "orig_colors = [\"darkblue\"] * len(x)\n",
    "abl_colors = [\"blue\"] * len(x)\n",
    "\n",
    "correct_next_str_tok = \" heated\"\n",
    "correct_next_token = model.to_single_token(\" heated\")\n",
    "# if repr(correct_next_str_tok) in x:\n",
    "#     idx = x.index(repr(correct_next_str_tok))\n",
    "#     orig_colors[idx] = \"darkgreen\"\n",
    "#     abl_colors[idx] = \"green\"\n",
    "\n",
    "x.append(repr(correct_next_str_tok))\n",
    "y_orig.append(logprobs_orig[correct_next_token].item())\n",
    "y_abl.append(logprobs_abl[correct_next_token].item())\n",
    "orig_colors.append(\"darkgreen\")\n",
    "abl_colors.append(\"green\")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Bar(x=x, y=y_orig, name='Original', marker_color=[\"#FF7700\"] * (len(x)-1) + [\"#024B7A\"]), # 7A30AB\n",
    "        go.Bar(x=x, y=y_abl, name='Ablated', marker_color=[\"#FFAE49\"] * (len(x)-1) + [\"#44A5C2\"]), # D44BFA\n",
    "    ],\n",
    "    # data = [\n",
    "    #     go.Bar(x=x[:-1], y=y_orig[:-1], name='Original', marker_color=\"#FF7700\", legendgroup=\"group1\"),\n",
    "    #     go.Bar(x=x[:-1], y=y_abl[:-1], name='Ablated', marker_color=\"#FFAE49\", legendgroup=\"group1\"),\n",
    "    #     go.Bar(x=[x[-1]], y=[y_orig[-1]], name='Original (correct token)', marker_color=\"#024B7A\", legendgroup=\"group2\"),\n",
    "    #     go.Bar(x=[x[-1]], y=[y_abl[-1]], name='Ablated (correct token)', marker_color=\"#44A5C2\", legendgroup=\"group2\"),\n",
    "    # ],\n",
    "    layout = dict(\n",
    "        barmode='group',\n",
    "        xaxis_tickangle=30,\n",
    "        title=\"Logprobs: original vs ablated\",\n",
    "        xaxis_title_text=\"Predicted next token\",\n",
    "        yaxis_title_text=\"Logprob\",\n",
    "        width=800,\n",
    "        bargap=0.35,\n",
    "    )\n",
    ")\n",
    "fig.data = fig.data #+ ({\"name\": \"New\"},)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_matrix_histogram(W_EE_dict, \" device\", k=10, include=[\" devices\"], circuit=\"OV\", neg=True, head=(10, 7))\n",
    "plot_full_matrix_histogram(W_EE_dict, \" devices\", k=10, include=[\" device\"], circuit=\"QK\", neg=False, head=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE = W_EE_dict[\"W_E (including MLPs)\"]\n",
    "W_EE = W_EE_dict[\"W_E (only MLPs)\"]\n",
    "W_U = W_EE_dict[\"W_U\"].T\n",
    "\n",
    "tok_strs = [\"pier\"]\n",
    "for i in range(len(tok_strs)): tok_strs.append(tok_strs[i].capitalize())\n",
    "for i in range(len(tok_strs)): tok_strs.append(tok_strs[i] + \"s\")\n",
    "for i in range(len(tok_strs)): tok_strs.append(\" \" + tok_strs[i])\n",
    "tok_strs = [s for s in tok_strs if model.to_tokens(s, prepend_bos=False).squeeze().ndim == 0]\n",
    "\n",
    "toks = model.to_tokens(tok_strs, prepend_bos=False).squeeze()\n",
    "\n",
    "W_EE_toks = W_EE[toks]\n",
    "W_EE_normed = W_EE_toks / W_EE_toks.norm(dim=-1, keepdim=True)\n",
    "cos_sim_embeddings = W_EE_normed @ W_EE_normed.T\n",
    "\n",
    "W_U_toks = W_U.T[toks]\n",
    "W_U_normed = W_U_toks / W_U_toks.norm(dim=-1, keepdim=True)\n",
    "cos_sim_unembeddings = W_U_normed @ W_U_normed.T\n",
    "\n",
    "W_EE_OV_toks_107 = W_EE_toks @ model.W_V[10, 7] @ model.W_O[10, 7]\n",
    "W_EE_OV_toks_99 = W_EE_toks @ model.W_V[9, 9] @ model.W_O[9, 9]\n",
    "W_EE_OV_toks_107_normed = W_EE_OV_toks_107 / W_EE_OV_toks_107.norm(dim=-1, keepdim=True)\n",
    "W_EE_OV_toks_99_normed = W_EE_OV_toks_99 / W_EE_OV_toks_99.norm(dim=-1, keepdim=True)\n",
    "cos_sim_107 = W_EE_OV_toks_107_normed @ W_EE_OV_toks_107_normed.T\n",
    "cos_sim_99 = W_EE_OV_toks_99_normed @ W_EE_OV_toks_99_normed.T\n",
    "\n",
    "imshow(\n",
    "    t.stack([\n",
    "        cos_sim_embeddings,\n",
    "        cos_sim_unembeddings,\n",
    "        cos_sim_107,\n",
    "        cos_sim_99,\n",
    "    ]),\n",
    "    x = list(map(repr, tok_strs)),\n",
    "    y = list(map(repr, tok_strs)),\n",
    "    title = \"Cosine similarity of variants of ' pier'\",\n",
    "    facet_col = 0,\n",
    "    facet_labels = [\"Effective embeddings\", \"Unembeddings\", \"W_OV output (10.7)\", \"W_OV output (9.9)\"],\n",
    "    border = True,\n",
    "    width=1200,\n",
    ")\n",
    "\n",
    "# W_EE_OV_normed = W_EE_OV / W_EE_OV.std(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create OV and QK circuits Streamlit page\n",
    "\n",
    "I need to save the following things:\n",
    "\n",
    "* The QK and OV matrices for head 10.7 and 11.10\n",
    "* The extended embedding and unembedding matrices\n",
    "* The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_store = {\n",
    "    \"tokenizer\": model.tokenizer,\n",
    "    \"W_V_107\": model.W_V[10, 7],\n",
    "    \"W_O_107\": model.W_O[10, 7],\n",
    "    \"W_V_1110\": model.W_V[11, 10],\n",
    "    \"W_O_1110\": model.W_O[11, 10],\n",
    "    \"W_Q_107\": model.W_Q[10, 7],\n",
    "    \"W_K_107\": model.W_K[10, 7],\n",
    "    \"W_Q_1110\": model.W_Q[11, 10],\n",
    "    \"W_K_1110\": model.W_K[11, 10],\n",
    "    \"b_Q_107\": model.b_Q[10, 7],\n",
    "    \"b_K_107\": model.b_K[10, 7],\n",
    "    \"b_Q_1110\": model.b_Q[11, 10],\n",
    "    \"b_K_1110\": model.b_K[11, 10],\n",
    "    \"W_EE\": W_EE_dict[\"W_E (including MLPs)\"],\n",
    "    \"W_U\": model.W_U,\n",
    "}\n",
    "dict_to_store = {k: v.half() if isinstance(v, t.Tensor) else v for k, v in dict_to_store.items()}\n",
    "\n",
    "with gzip.open(_ST_HTML_PATH / f\"OV_QK_circuits.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_to_store, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate `explore_prompts` HTML plots for Streamlit page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PLOTS = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS,\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = True,\n",
    "    progress_bar = True,\n",
    "    restrict_computation = [\"LOSS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    model.W_U.T[model.to_single_token(\" pier\")].norm().item(), \n",
    "    model.W_U.T[model.to_single_token(\" Pier\")].norm().item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    model.W_U.T[model.to_single_token(\" pier\")],\n",
    "    model.W_U.T[model.to_single_token(\" Pier\")],\n",
    "    dim=-1\n",
    ").item()\n",
    "\n",
    "\n",
    "pier = model.W_U.T[model.to_single_token(\" pier\")]\n",
    "Pier = model.W_U.T[model.to_single_token(\" Pier\")]\n",
    "pier /= pier.norm()\n",
    "Pier /= Pier.norm()\n",
    "print(pier @ Pier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_U(s):\n",
    "    return model.W_U.T[model.to_single_token(s)]\n",
    "def W_EE0(s):\n",
    "    return W_EE_dict[\"W_E (only MLPs)\"][model.to_single_token(s)]\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return v1 @ v2 / (v1.norm() * v2.norm())\n",
    "\n",
    "print(f\"Unembeddings cosine similarity (Berkeley) = {cos_sim(W_U('keley'), W_U(' Berkeley')):.3f}\") \n",
    "print(f\"Embeddings cosine similarity (Berkeley)   = {cos_sim(W_EE0('keley'), W_EE0(' Berkeley')):.3f}\") \n",
    "print(\"\")\n",
    "print(f\"Unembeddings cosine similarity (pier) = {cos_sim(W_U(' pier'), W_U(' Pier')):.3f}\") \n",
    "print(f\"Embeddings cosine similarity (pier)   = {cos_sim(W_EE0(' pier'), W_EE0(' Pier')):.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    W_EE0(\" screen\") - W_EE0(\" screens\"),\n",
    "    W_EE0(\" device\") - W_EE0(\" devices\"),\n",
    "    dim=-1\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    W_EE(\" computer\") - W_EE(\" computers\"),\n",
    "    W_EE(\" sign\") - W_EE(\" signs\"),\n",
    "    dim=-1\n",
    ").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots of copy-suppression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICS = MODEL_RESULTS.is_copy_suppression[(\"direct\", \"frozen\", \"mean\")][10, 7]\n",
    "z = ICS[\"CS\"]\n",
    "ratio = ICS[\"LR\"]\n",
    "l_orig = ICS[\"L_ORIG\"]\n",
    "l_cs = ICS[\"L_CS\"]\n",
    "l_abl = ICS[\"L_ABL\"]\n",
    "# Get the 2.5% cutoff examples, and the 5% cutoff examples\n",
    "l_abl_minus_orig = l_abl - l_orig\n",
    "\n",
    "non_extreme_05pct = (l_abl_minus_orig < l_abl_minus_orig.quantile(0.95)) & (l_abl_minus_orig > l_abl_minus_orig.quantile(0.05))\n",
    "non_extreme_025pct = (l_abl_minus_orig < l_abl_minus_orig.quantile(0.975)) & (l_abl_minus_orig > l_abl_minus_orig.quantile(0.025))\n",
    "\n",
    "z_05pct = t.where(non_extreme_05pct, 2, z)\n",
    "z_025pct = t.where(non_extreme_025pct, 2, z)\n",
    "\n",
    "ratio_05pct = ratio.flatten()[z_05pct.flatten() != 2]\n",
    "ratio_025pct = ratio.flatten()[z_025pct.flatten() != 2]\n",
    "\n",
    "fig = hist(\n",
    "    ratio_05pct,\n",
    "    template=\"simple_white\",\n",
    "    title=create_title_and_subtitles(\n",
    "        title=\"Cross-entropy loss, when ablating everything except copy-suppression mechanism\",\n",
    "        subtitles=[\n",
    "            \"One = same loss as original (no ablation)\",\n",
    "            \"Zero = same loss as complete ablation\",\n",
    "            \"Only looking at top/bottom 5% of loss-affecting examples\"\n",
    "        ]\n",
    "    ),\n",
    "    labels={\"x\": \"Cross entropy loss (post-affine transformation)\"},\n",
    "    return_fig=True,\n",
    ")\n",
    "fig.update_layout(title_y=0.92, margin_t=150, height=500, width=800)\n",
    "fig.add_vline(opacity=1.0, x=ratio_05pct.median(), line=dict(width=3, color=\"red\"), annotation_text=f\"Median = {ratio_05pct.median():.3f} \", annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = hist(\n",
    "    ratio_025pct,\n",
    "    template=\"simple_white\",\n",
    "    title=create_title_and_subtitles(\n",
    "        title=\"Cross-entropy loss, when ablating everything except copy-suppression mechanism\",\n",
    "        subtitles=[\n",
    "            \"One = same loss as original (no ablation)\",\n",
    "            \"Zero = same loss as complete ablation\",\n",
    "            \"Only looking at top/bottom 2.5% of loss-affecting examples\"\n",
    "        ]\n",
    "    ),\n",
    "    labels={\"x\": \"Cross entropy loss (post-affine transformation)\"},\n",
    "    return_fig=True,\n",
    ")\n",
    "fig.update_layout(title_y=0.92, margin_t=150, height=500, width=800)\n",
    "fig.add_vline(opacity=1.0, x=ratio_025pct.median(), line=dict(width=3, color=\"red\"), annotation_text=f\"Median = {ratio_025pct.median():.3f} \", annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [02] Copy Suppression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('/home/ubuntu/TransformerLens/transformer_lens/rs/callum2/explore_prompts/media/')\n",
    "ICS_list = [\n",
    "    pickle.load(open(p / f\"ICS_0{i}.pkl\", \"rb\"))\n",
    "    for i in range(3)\n",
    "]\n",
    "# ICS_list = [MODEL_RESULTS.is_copy_suppression[(\"direct\", \"frozen\", \"mean\")][10, 7]]\n",
    "\n",
    "generate_scatter(\n",
    "    ICS_list=ICS_list,\n",
    "    DATA_STR_TOKS_PARSED_list=[DATA_STR_TOKS_PARSED_1, DATA_STR_TOKS_PARSED_2, DATA_STR_TOKS_PARSED_3],\n",
    "    head=(10, 7),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
